{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/research/COS513_project/src\n",
      "Error: encoder not found!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(),\"..\"))\n",
    "print(path)\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from dynamics_predict.dynamics_networks import DynamicsNetwork, DynamicsParamsOptimizer, EncoderDynamicsNetwork, EncoderDecoderDynamicsNetwork, VAEDynamicsNetwork\n",
    "from rl.policy_networks import DPG_PolicyNetwork\n",
    "from utils.load_params import load_params\n",
    "from utils.common_func import rand_params\n",
    "from dynamics_predict.defaults import DYNAMICS_PARAMS, HYPER_PARAMS\n",
    "from environment import envs\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter dimension:  5\n"
     ]
    }
   ],
   "source": [
    "env_name = 'inverteddoublependulum'\n",
    "data_path = path+'/data/dynamics_data/'+env_name+'/dynamics.npy'\n",
    "param_dim = len(DYNAMICS_PARAMS[env_name+'dynamics'])\n",
    "print('parameter dimension: ', param_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in dest data:  5823798\n",
      "(100000, 11) (100000, 1) (100000, 5) (100000, 11)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(data_path, allow_pickle=True)\n",
    "print('number of samples in dest data: ', len(train_data))\n",
    "# split data\n",
    "def split_data(data, partial=100000):\n",
    "    data_s, data_a, data_param, data_s_ = [], [], [], []\n",
    "    for d in data[:partial]:\n",
    "        [s,a,param], s_ = d\n",
    "        data_s.append(s)\n",
    "        data_a.append(a)\n",
    "        data_param.append(param)\n",
    "        data_s_.append(s_)\n",
    "\n",
    "    data_s = np.array(data_s)\n",
    "    data_a = np.array(data_a)\n",
    "    data_param = np.array(data_param)\n",
    "    data_s_ = np.array(data_s_)\n",
    "    \n",
    "    return data_s, data_a, data_param, data_s_\n",
    "\n",
    "data_s, data_a, data_param, data_s_ = split_data(train_data)\n",
    "print(data_s.shape, data_a.shape, data_param.shape, data_s_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch Linear Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "class DynamicsParamsOptimizer():\n",
    "    \"\"\" \n",
    "    Dynamics parameters optimization model (gradient-based) based on a trained \n",
    "    forward dynamics prediction network: (s, a, learnable_params) -> s_ with real-world data. \n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, param_dim, latent_dim, switch_dim, model_save_path):\n",
    "        self.model = SLDynamicsNetwork(state_dim, action_dim, param_dim, latent_dim, switch_dim).to(device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.model_save_path = model_save_path\n",
    "\n",
    "    def train(self, s, a, theta, s_, epoch):\n",
    "        \"\"\" s,a concat with param (learnable) -> s_ \"\"\"\n",
    "        if not isinstance(s_, torch.Tensor):\n",
    "            s_ = torch.Tensor(s_).to(device)\n",
    "\n",
    "        for ep in range(epoch):\n",
    "            s_pred = self.model.forward(s, a, theta)\n",
    "            self.model.optimizer.zero_grad()\n",
    "            loss = self.criterion(s_pred, s_)\n",
    "            loss.backward()\n",
    "            self.model.optimizer.step()\n",
    "            if ep%100==0:\n",
    "                print('epoch: {}, loss: {}'.format(ep, loss.item()))\n",
    "                torch.save(self.model.state_dict(), self.model_save_path+'model')\n",
    "            \n",
    "\n",
    "class SLDynamicsNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, param_dim, latent_dim, switch_dim, lr=1e-4):\n",
    "        super(SLDynamicsNetwork, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.A = nn.Parameter(torch.rand((switch_dim, latent_dim, state_dim, state_dim)), requires_grad=True)\n",
    "        self.B = nn.Parameter(torch.rand((switch_dim, latent_dim, state_dim, action_dim)), requires_grad=True)\n",
    "        self.E = nn.Parameter(torch.rand((param_dim, latent_dim)), requires_grad=True)\n",
    "        self.switch_logits = nn.Sequential(\n",
    "            nn.Linear(state_dim, switch_dim, bias=False)  # only weight matrix, no bias\n",
    "        )\n",
    "        # print(dict(self.named_parameters()))\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def gaussian_noise(self, shape, scale):\n",
    "        normal = Normal(0, 1)\n",
    "        epsilon = scale * normal.sample(shape) \n",
    "        return epsilon\n",
    "\n",
    "    def get_switch_var(self, s):\n",
    "        logits_ = self.switch_logits(s)\n",
    "        switch_var = F.gumbel_softmax(logits_, tau=1, hard=True)  # if hard, return one-hot\n",
    "        return switch_var\n",
    "\n",
    "    def get_s_before_encode(self, s, a):\n",
    "        switch_var = self.get_switch_var(s)\n",
    "        A_w = torch.einsum('ab,bcde->acde', switch_var, self.A) # chosen by the switch variable; shape (#batch, #latent, #state, #state)\n",
    "        B_w = torch.einsum('ab,bcde->acde', switch_var, self.B) # chosen by the switch variable; shape (#batch, #latent, #state, #action)\n",
    "        s_before_encode = torch.einsum('abcd,ad->abc', A_w, s) + torch.einsum('abcd,ad->abc', B_w, a)  # shape (#batch, #latent, #state)\n",
    "        return s_before_encode\n",
    "\n",
    "    def forward(self, s, a, theta):\n",
    "        if not isinstance(s, torch.Tensor):\n",
    "            s = torch.Tensor(s).to(device)\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.Tensor(a).to(device)\n",
    "        if not isinstance(theta, torch.Tensor):\n",
    "            theta = torch.Tensor(theta).to(device)\n",
    "        batch_size = s.shape[0]\n",
    "\n",
    "        s_before_encode = self.get_s_before_encode(s, a)\n",
    "        s_before_noise = torch.einsum('ab,abc->ac', theta@self.E, s_before_encode)  # shape (#batch, #state)\n",
    "        noise = self.gaussian_noise(shape=(batch_size, self.state_dim), scale=0.)\n",
    "        s_ = s_before_noise + noise.to(device)\n",
    "\n",
    "        return s_\n",
    "\n",
    "    def get_latent_code(self, s, a, s_):\n",
    "        if not isinstance(s, torch.Tensor):\n",
    "            s = torch.Tensor(s).to(device).to(device)\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.Tensor(a).to(device)        \n",
    "        if not isinstance(s_, torch.Tensor):\n",
    "            s_ = torch.Tensor(s_).to(device)     \n",
    "\n",
    "        s_before_encode = self.get_s_before_encode(s, a)\n",
    "        inv_s = torch.linalg.pinv(s_before_encode)  # pseudo-inverse; shape (#batch, #state, #latent)\n",
    "        alpha = torch.einsum('ab,abc->ac', s_, inv_s)\n",
    "        print(s_before_encode.shape, inv_s.shape, alpha.shape)\n",
    "\n",
    "        return alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 537.4883422851562\n",
      "epoch: 100, loss: 497.87249755859375\n",
      "epoch: 200, loss: 461.5218505859375\n",
      "epoch: 300, loss: 428.9962158203125\n",
      "epoch: 400, loss: 398.29608154296875\n",
      "epoch: 500, loss: 370.56011962890625\n",
      "epoch: 600, loss: 345.722412109375\n",
      "epoch: 700, loss: 321.8437194824219\n",
      "epoch: 800, loss: 300.49273681640625\n",
      "epoch: 900, loss: 279.8143615722656\n",
      "epoch: 1000, loss: 260.5543212890625\n",
      "epoch: 1100, loss: 242.75592041015625\n",
      "epoch: 1200, loss: 226.7599334716797\n",
      "epoch: 1300, loss: 210.84959411621094\n",
      "epoch: 1400, loss: 196.90492248535156\n",
      "epoch: 1500, loss: 184.2673797607422\n",
      "epoch: 1600, loss: 171.970703125\n",
      "epoch: 1700, loss: 160.6292266845703\n",
      "epoch: 1800, loss: 149.6862335205078\n",
      "epoch: 1900, loss: 139.957275390625\n",
      "epoch: 2000, loss: 130.5864715576172\n",
      "epoch: 2100, loss: 122.14558410644531\n",
      "epoch: 2200, loss: 114.12090301513672\n",
      "epoch: 2300, loss: 106.89201354980469\n",
      "epoch: 2400, loss: 99.89753723144531\n",
      "epoch: 2500, loss: 93.7880859375\n",
      "epoch: 2600, loss: 87.22730255126953\n",
      "epoch: 2700, loss: 81.83631896972656\n",
      "epoch: 2800, loss: 76.72122955322266\n",
      "epoch: 2900, loss: 71.76036071777344\n",
      "epoch: 3000, loss: 66.85956573486328\n",
      "epoch: 3100, loss: 62.43924331665039\n",
      "epoch: 3200, loss: 58.64507293701172\n",
      "epoch: 3300, loss: 54.93611145019531\n",
      "epoch: 3400, loss: 51.145591735839844\n",
      "epoch: 3500, loss: 48.05408477783203\n",
      "epoch: 3600, loss: 44.7950325012207\n",
      "epoch: 3700, loss: 41.898311614990234\n",
      "epoch: 3800, loss: 39.351280212402344\n",
      "epoch: 3900, loss: 36.57415771484375\n",
      "epoch: 4000, loss: 34.33924865722656\n",
      "epoch: 4100, loss: 32.35869216918945\n",
      "epoch: 4200, loss: 30.15932273864746\n",
      "epoch: 4300, loss: 28.204334259033203\n",
      "epoch: 4400, loss: 26.17108726501465\n",
      "epoch: 4500, loss: 24.49980926513672\n",
      "epoch: 4600, loss: 23.063308715820312\n",
      "epoch: 4700, loss: 21.54859161376953\n",
      "epoch: 4800, loss: 20.141889572143555\n",
      "epoch: 4900, loss: 18.609405517578125\n",
      "epoch: 5000, loss: 17.323259353637695\n",
      "epoch: 5100, loss: 16.40073585510254\n",
      "epoch: 5200, loss: 15.233479499816895\n",
      "epoch: 5300, loss: 14.228206634521484\n",
      "epoch: 5400, loss: 13.141356468200684\n",
      "epoch: 5500, loss: 12.274105072021484\n",
      "epoch: 5600, loss: 11.55132007598877\n",
      "epoch: 5700, loss: 10.711014747619629\n",
      "epoch: 5800, loss: 10.11236572265625\n",
      "epoch: 5900, loss: 9.38266372680664\n",
      "epoch: 6000, loss: 8.674667358398438\n",
      "epoch: 6100, loss: 8.102201461791992\n",
      "epoch: 6200, loss: 7.582343101501465\n",
      "epoch: 6300, loss: 7.163240432739258\n",
      "epoch: 6400, loss: 6.639410495758057\n",
      "epoch: 6500, loss: 6.1713643074035645\n",
      "epoch: 6600, loss: 5.84082555770874\n",
      "epoch: 6700, loss: 5.4531965255737305\n",
      "epoch: 6800, loss: 5.111390590667725\n",
      "epoch: 6900, loss: 4.875304222106934\n",
      "epoch: 7000, loss: 4.524689197540283\n",
      "epoch: 7100, loss: 4.221683979034424\n",
      "epoch: 7200, loss: 3.982189893722534\n",
      "epoch: 7300, loss: 3.6834256649017334\n",
      "epoch: 7400, loss: 3.5212535858154297\n",
      "epoch: 7500, loss: 3.2906787395477295\n",
      "epoch: 7600, loss: 3.0716753005981445\n",
      "epoch: 7700, loss: 2.9062557220458984\n",
      "epoch: 7800, loss: 2.769529104232788\n",
      "epoch: 7900, loss: 2.597111701965332\n",
      "epoch: 8000, loss: 2.4582154750823975\n",
      "epoch: 8100, loss: 2.328871488571167\n",
      "epoch: 8200, loss: 2.1729166507720947\n",
      "epoch: 8300, loss: 2.0810956954956055\n",
      "epoch: 8400, loss: 1.9307771921157837\n",
      "epoch: 8500, loss: 1.8348356485366821\n",
      "epoch: 8600, loss: 1.7517213821411133\n",
      "epoch: 8700, loss: 1.6564974784851074\n",
      "epoch: 8800, loss: 1.5658024549484253\n",
      "epoch: 8900, loss: 1.4745956659317017\n",
      "epoch: 9000, loss: 1.3857066631317139\n",
      "epoch: 9100, loss: 1.3342097997665405\n",
      "epoch: 9200, loss: 1.2447361946105957\n",
      "epoch: 9300, loss: 1.178697943687439\n",
      "epoch: 9400, loss: 1.1260613203048706\n",
      "epoch: 9500, loss: 1.0738880634307861\n",
      "epoch: 9600, loss: 1.0208545923233032\n",
      "epoch: 9700, loss: 0.9557663202285767\n",
      "epoch: 9800, loss: 0.8926223516464233\n",
      "epoch: 9900, loss: 0.8508350253105164\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# train\n",
    "state_dim = data_s.shape[1]\n",
    "action_dim = data_a.shape[1]\n",
    "param_dim = data_param.shape[1]\n",
    "latent_dim = 2\n",
    "switch_dim = 5\n",
    "\n",
    "model_save_path = f'../data/weights/dynamics/inverteddoublependulum/'\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "updater = DynamicsParamsOptimizer(state_dim, action_dim, param_dim, latent_dim, switch_dim, model_save_path)\n",
    "updater.train(data_s, data_a, data_param, data_s_, epoch=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in dest data:  10\n",
      "(275717, 11) (275717, 1) (5,) (275717, 11)\n",
      "torch.Size([10, 2, 11]) torch.Size([10, 11, 2]) torch.Size([10, 2])\n",
      "tensor([[7.5096, 4.9735],\n",
      "        [4.3295, 3.0814],\n",
      "        [4.9684, 3.5469],\n",
      "        [4.2286, 2.6858],\n",
      "        [3.8032, 2.0696],\n",
      "        [5.6362, 3.6489],\n",
      "        [4.0966, 2.6322],\n",
      "        [4.3626, 2.9226],\n",
      "        [7.1042, 5.1002],\n",
      "        [0.8724, 0.5177]], device='cuda:0', grad_fn=<ViewBackward>) tensor([4.6911, 3.1179], device='cuda:0', grad_fn=<MeanBackward1>) tensor([[5.5475, 3.7723]], device='cuda:0', grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "test_data_path = path+'/data/dynamics_data/'+env_name+'/test_dynamics.npy'\n",
    "test_data = np.load(test_data_path, allow_pickle=True)\n",
    "print('number of samples in dest data: ', len(test_data))\n",
    "idx=5  # index of sample to test: 0-10\n",
    "test_s = np.array(test_data[idx]['sa'])[:, :-1]\n",
    "test_a = np.array(test_data[idx]['sa'])[:, -1:]\n",
    "test_param = np.array(test_data[idx]['params'])\n",
    "test_s_ = np.array(test_data[idx]['s_'])\n",
    "print(test_s.shape, test_a.shape, test_param.shape, test_s_.shape)\n",
    "\n",
    "partial = 10\n",
    "alpha = updater.model.get_latent_code(test_s[:partial], test_a[:partial], test_s_[:partial])\n",
    "\n",
    "# compare with encoded value\n",
    "alpha_ = torch.Tensor([test_param]).to(device)@updater.model.E\n",
    "print(alpha, alpha.mean(dim=0), alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 11]) torch.Size([20, 11, 2]) torch.Size([20, 2])\n",
      "tensor([[ 2.2643, -1.8454],\n",
      "        [ 2.8811, -2.2793],\n",
      "        [ 3.5958, -2.9697],\n",
      "        [ 1.7742, -1.4963],\n",
      "        [ 2.4365, -2.0887],\n",
      "        [ 2.2297, -1.8889],\n",
      "        [ 1.8014, -1.4508],\n",
      "        [ 2.7828, -2.4257],\n",
      "        [ 3.4125, -2.8278],\n",
      "        [ 1.7107, -1.3469],\n",
      "        [ 2.0031, -1.6024],\n",
      "        [ 2.5392, -2.0994],\n",
      "        [ 3.7406, -3.1136],\n",
      "        [ 1.4040, -1.1620],\n",
      "        [ 2.3428, -1.9775],\n",
      "        [ 4.2024, -3.5203],\n",
      "        [ 1.4234, -1.1618],\n",
      "        [ 1.6157, -1.2830],\n",
      "        [ 1.8207, -1.4661],\n",
      "        [ 2.3034, -1.9059]], grad_fn=<ViewBackward>) tensor([[ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# test on train data\n",
    "test_size = 20\n",
    "test_s = data_s[:test_size]\n",
    "test_a = data_a[:test_size]\n",
    "test_param = data_param[:test_size]\n",
    "test_s_ = data_s_[:test_size]\n",
    "\n",
    "alpha = updater.model.get_latent_code(test_s, test_a, test_s_)\n",
    "\n",
    "# compare with encoded value\n",
    "alpha_ = torch.Tensor(test_param)@updater.model.E\n",
    "print(alpha, alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0034d21db9019aa9fe47b3d79d474777c42bec03792ede4a8b556d1ed67f2d91"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
