{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/research/COS513_project/src\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(),\"..\"))\n",
    "print(path)\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from dynamics_predict.dynamics_networks import DynamicsNetwork, EncoderDynamicsNetwork, EncoderDecoderDynamicsNetwork, VAEDynamicsNetwork\n",
    "from rl.policy_networks import DPG_PolicyNetwork\n",
    "from utils.load_params import load_params\n",
    "from utils.common_func import rand_params\n",
    "from dynamics_predict.defaults import DYNAMICS_PARAMS, HYPER_PARAMS\n",
    "from environment import envs\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter dimension:  5\n"
     ]
    }
   ],
   "source": [
    "env_name = 'inverteddoublependulum'\n",
    "data_path = path+'/data/dynamics_data/'+env_name+'/dynamics.npy'\n",
    "param_dim = len(DYNAMICS_PARAMS[env_name+'dynamics'])\n",
    "print('parameter dimension: ', param_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in dest data:  5823798\n",
      "(100000, 11) (100000, 1) (100000, 5) (100000, 11)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(data_path, allow_pickle=True)\n",
    "print('number of samples in test data: ', len(train_data))\n",
    "# split data\n",
    "def split_data(data, partial=100000):\n",
    "    data_s, data_a, data_param, data_s_ = [], [], [], []\n",
    "    for d in data[:partial]:\n",
    "        [s,a,param], s_ = d\n",
    "        data_s.append(s)\n",
    "        data_a.append(a)\n",
    "        data_param.append(param)\n",
    "        data_s_.append(s_)\n",
    "\n",
    "    data_s = np.array(data_s)\n",
    "    data_a = np.array(data_a)\n",
    "    data_param = np.array(data_param)\n",
    "    data_s_ = np.array(data_s_)\n",
    "    \n",
    "    return data_s, data_a, data_param, data_s_\n",
    "\n",
    "data_s, data_a, data_param, data_s_ = split_data(train_data)\n",
    "print(data_s.shape, data_a.shape, data_param.shape, data_s_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = data_s.shape[1]\n",
    "action_dim = data_a.shape[1]\n",
    "param_dim = data_param.shape[1]\n",
    "latent_dim = 2\n",
    "switch_dim = 5\n",
    "\n",
    "model_save_path = f'../data/weights/dynamics/inverteddoublependulum/'\n",
    "os.makedirs(model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch Linear Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "class DynamicsParamsOptimizer():\n",
    "    \"\"\" \n",
    "    Dynamics parameters optimization model (gradient-based) based on a trained \n",
    "    forward dynamics prediction network: (s, a, learnable_params) -> s_ with real-world data. \n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, param_dim, latent_dim, switch_dim, model_save_path):\n",
    "        self.model = SLDynamicsNetwork(state_dim, action_dim, param_dim, latent_dim, switch_dim).to(device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.model_save_path = model_save_path\n",
    "\n",
    "    def train(self, s, a, theta, s_, epoch):\n",
    "        \"\"\" s,a concat with param (learnable) -> s_ \"\"\"\n",
    "        if not isinstance(s_, torch.Tensor):\n",
    "            s_ = torch.Tensor(s_).to(device)\n",
    "\n",
    "        for ep in range(epoch):\n",
    "            s_pred = self.model.forward(s, a, theta)\n",
    "            self.model.optimizer.zero_grad()\n",
    "            loss = self.criterion(s_pred, s_)\n",
    "            loss.backward()\n",
    "            self.model.optimizer.step()\n",
    "            if ep%100==0:\n",
    "                print('epoch: {}, loss: {}'.format(ep, loss.item()))\n",
    "                torch.save(self.model.state_dict(), self.model_save_path+'model')\n",
    "            \n",
    "\n",
    "class SLDynamicsNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, param_dim, latent_dim, switch_dim, lr=1e-4):\n",
    "        super(SLDynamicsNetwork, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.A = nn.Parameter(torch.rand((switch_dim, latent_dim, state_dim, state_dim)), requires_grad=True)\n",
    "        self.B = nn.Parameter(torch.rand((switch_dim, latent_dim, state_dim, action_dim)), requires_grad=True)\n",
    "        self.E = nn.Parameter(torch.rand((param_dim, latent_dim)), requires_grad=True)\n",
    "        self.switch_logits = nn.Sequential(\n",
    "            nn.Linear(state_dim, switch_dim, bias=False)  # only weight matrix, no bias\n",
    "        )\n",
    "        # print(dict(self.named_parameters()))\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def gaussian_noise(self, shape, scale):\n",
    "        normal = Normal(0, 1)\n",
    "        epsilon = scale * normal.sample(shape) \n",
    "        return epsilon\n",
    "\n",
    "    def get_switch_var(self, s):\n",
    "        logits_ = self.switch_logits(s)\n",
    "        switch_var = F.gumbel_softmax(logits_, tau=1, hard=True)  # if hard, return one-hot\n",
    "        return switch_var\n",
    "\n",
    "    def get_s_before_encode(self, s, a):\n",
    "        switch_var = self.get_switch_var(s)\n",
    "        A_w = torch.einsum('ab,bcde->acde', switch_var, self.A) # chosen by the switch variable; shape (#batch, #latent, #state, #state)\n",
    "        B_w = torch.einsum('ab,bcde->acde', switch_var, self.B) # chosen by the switch variable; shape (#batch, #latent, #state, #action)\n",
    "        s_before_encode = torch.einsum('abcd,ad->abc', A_w, s) + torch.einsum('abcd,ad->abc', B_w, a)  # shape (#batch, #latent, #state)\n",
    "        return s_before_encode\n",
    "\n",
    "    def forward(self, s, a, theta):\n",
    "        if not isinstance(s, torch.Tensor):\n",
    "            s = torch.Tensor(s).to(device)\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.Tensor(a).to(device)\n",
    "        if not isinstance(theta, torch.Tensor):\n",
    "            theta = torch.Tensor(theta).to(device)\n",
    "        batch_size = s.shape[0]\n",
    "\n",
    "        s_before_encode = self.get_s_before_encode(s, a)\n",
    "        s_before_noise = torch.einsum('ab,abc->ac', theta@self.E, s_before_encode)  # shape (#batch, #state)\n",
    "        noise = self.gaussian_noise(shape=(batch_size, self.state_dim), scale=0.)\n",
    "        s_ = s_before_noise + noise.to(device)\n",
    "\n",
    "        return s_\n",
    "\n",
    "    def get_latent_code(self, s, a, s_):\n",
    "        if not isinstance(s, torch.Tensor):\n",
    "            s = torch.Tensor(s).to(device).to(device)\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.Tensor(a).to(device)        \n",
    "        if not isinstance(s_, torch.Tensor):\n",
    "            s_ = torch.Tensor(s_).to(device)     \n",
    "\n",
    "        s_before_encode = self.get_s_before_encode(s, a)\n",
    "        inv_s = torch.linalg.pinv(s_before_encode)  # pseudo-inverse; shape (#batch, #state, #latent)\n",
    "        alpha = torch.einsum('ab,abc->ac', s_, inv_s)\n",
    "        print(s_before_encode.shape, inv_s.shape, alpha.shape)\n",
    "\n",
    "        return alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 324.5203857421875\n",
      "epoch: 100, loss: 298.9375305175781\n",
      "epoch: 200, loss: 276.59454345703125\n",
      "epoch: 300, loss: 255.57992553710938\n",
      "epoch: 400, loss: 236.47393798828125\n",
      "epoch: 500, loss: 219.1861572265625\n",
      "epoch: 600, loss: 203.21389770507812\n",
      "epoch: 700, loss: 188.7410888671875\n",
      "epoch: 800, loss: 175.6169891357422\n",
      "epoch: 900, loss: 163.41543579101562\n",
      "epoch: 1000, loss: 152.49801635742188\n",
      "epoch: 1100, loss: 141.8196258544922\n",
      "epoch: 1200, loss: 131.9557647705078\n",
      "epoch: 1300, loss: 122.83441162109375\n",
      "epoch: 1400, loss: 114.36428833007812\n",
      "epoch: 1500, loss: 106.8127670288086\n",
      "epoch: 1600, loss: 99.80718994140625\n",
      "epoch: 1700, loss: 92.93031311035156\n",
      "epoch: 1800, loss: 86.51558685302734\n",
      "epoch: 1900, loss: 80.93875885009766\n",
      "epoch: 2000, loss: 75.38483428955078\n",
      "epoch: 2100, loss: 70.01204681396484\n",
      "epoch: 2200, loss: 65.4825439453125\n",
      "epoch: 2300, loss: 61.2170295715332\n",
      "epoch: 2400, loss: 57.09694290161133\n",
      "epoch: 2500, loss: 53.214298248291016\n",
      "epoch: 2600, loss: 49.67768859863281\n",
      "epoch: 2700, loss: 46.377384185791016\n",
      "epoch: 2800, loss: 43.30165100097656\n",
      "epoch: 2900, loss: 40.230648040771484\n",
      "epoch: 3000, loss: 37.60527801513672\n",
      "epoch: 3100, loss: 34.909271240234375\n",
      "epoch: 3200, loss: 32.54482650756836\n",
      "epoch: 3300, loss: 30.25592803955078\n",
      "epoch: 3400, loss: 28.1134090423584\n",
      "epoch: 3500, loss: 26.282894134521484\n",
      "epoch: 3600, loss: 24.222320556640625\n",
      "epoch: 3700, loss: 22.62782859802246\n",
      "epoch: 3800, loss: 21.02157211303711\n",
      "epoch: 3900, loss: 19.33570671081543\n",
      "epoch: 4000, loss: 17.98805809020996\n",
      "epoch: 4100, loss: 16.759662628173828\n",
      "epoch: 4200, loss: 15.254434585571289\n",
      "epoch: 4300, loss: 14.164440155029297\n",
      "epoch: 4400, loss: 13.120105743408203\n",
      "epoch: 4500, loss: 12.0531644821167\n",
      "epoch: 4600, loss: 11.05904769897461\n",
      "epoch: 4700, loss: 10.121085166931152\n",
      "epoch: 4800, loss: 9.37381649017334\n",
      "epoch: 4900, loss: 8.531477928161621\n",
      "epoch: 5000, loss: 7.81948709487915\n",
      "epoch: 5100, loss: 7.224096775054932\n",
      "epoch: 5200, loss: 6.563879013061523\n",
      "epoch: 5300, loss: 5.988875389099121\n",
      "epoch: 5400, loss: 5.507105350494385\n",
      "epoch: 5500, loss: 5.061393737792969\n",
      "epoch: 5600, loss: 4.63860559463501\n",
      "epoch: 5700, loss: 4.251425266265869\n",
      "epoch: 5800, loss: 3.840623140335083\n",
      "epoch: 5900, loss: 3.5328991413116455\n",
      "epoch: 6000, loss: 3.2560341358184814\n",
      "epoch: 6100, loss: 2.978693723678589\n",
      "epoch: 6200, loss: 2.7413570880889893\n",
      "epoch: 6300, loss: 2.4790403842926025\n",
      "epoch: 6400, loss: 2.2882614135742188\n",
      "epoch: 6500, loss: 2.073394298553467\n",
      "epoch: 6600, loss: 1.9166908264160156\n",
      "epoch: 6700, loss: 1.7572170495986938\n",
      "epoch: 6800, loss: 1.5977978706359863\n",
      "epoch: 6900, loss: 1.4884068965911865\n",
      "epoch: 7000, loss: 1.3559706211090088\n",
      "epoch: 7100, loss: 1.2485840320587158\n",
      "epoch: 7200, loss: 1.1302839517593384\n",
      "epoch: 7300, loss: 1.0574249029159546\n",
      "epoch: 7400, loss: 0.9675189256668091\n",
      "epoch: 7500, loss: 0.9021834135055542\n",
      "epoch: 7600, loss: 0.8404320478439331\n",
      "epoch: 7700, loss: 0.7823190689086914\n",
      "epoch: 7800, loss: 0.700934648513794\n",
      "epoch: 7900, loss: 0.6409755349159241\n",
      "epoch: 8000, loss: 0.6062850952148438\n",
      "epoch: 8100, loss: 0.5610832571983337\n",
      "epoch: 8200, loss: 0.5168686509132385\n",
      "epoch: 8300, loss: 0.4888532757759094\n",
      "epoch: 8400, loss: 0.46014848351478577\n",
      "epoch: 8500, loss: 0.4421604871749878\n",
      "epoch: 8600, loss: 0.3951645493507385\n",
      "epoch: 8700, loss: 0.3852694034576416\n",
      "epoch: 8800, loss: 0.34175777435302734\n",
      "epoch: 8900, loss: 0.33020493388175964\n",
      "epoch: 9000, loss: 0.3000223636627197\n",
      "epoch: 9100, loss: 0.2911267578601837\n",
      "epoch: 9200, loss: 0.27181267738342285\n",
      "epoch: 9300, loss: 0.25852254033088684\n",
      "epoch: 9400, loss: 0.2483808696269989\n",
      "epoch: 9500, loss: 0.235544815659523\n",
      "epoch: 9600, loss: 0.21918347477912903\n",
      "epoch: 9700, loss: 0.20913544297218323\n",
      "epoch: 9800, loss: 0.20283329486846924\n",
      "epoch: 9900, loss: 0.1889834851026535\n",
      "epoch: 10000, loss: 0.18368980288505554\n",
      "epoch: 10100, loss: 0.1760396808385849\n",
      "epoch: 10200, loss: 0.16563889384269714\n",
      "epoch: 10300, loss: 0.15771979093551636\n",
      "epoch: 10400, loss: 0.1581100970506668\n",
      "epoch: 10500, loss: 0.1466110199689865\n",
      "epoch: 10600, loss: 0.14812994003295898\n",
      "epoch: 10700, loss: 0.13691411912441254\n",
      "epoch: 10800, loss: 0.13617759943008423\n",
      "epoch: 10900, loss: 0.13171134889125824\n",
      "epoch: 11000, loss: 0.1277608722448349\n",
      "epoch: 11100, loss: 0.12424955517053604\n",
      "epoch: 11200, loss: 0.12241915613412857\n",
      "epoch: 11300, loss: 0.1171136349439621\n",
      "epoch: 11400, loss: 0.11676374077796936\n",
      "epoch: 11500, loss: 0.10957759618759155\n",
      "epoch: 11600, loss: 0.11006448417901993\n",
      "epoch: 11700, loss: 0.10944127291440964\n",
      "epoch: 11800, loss: 0.10957811772823334\n",
      "epoch: 11900, loss: 0.10330279916524887\n",
      "epoch: 12000, loss: 0.10311975330114365\n",
      "epoch: 12100, loss: 0.10014992952346802\n",
      "epoch: 12200, loss: 0.09758023917675018\n",
      "epoch: 12300, loss: 0.09813296794891357\n",
      "epoch: 12400, loss: 0.09525542706251144\n",
      "epoch: 12500, loss: 0.09446270018815994\n",
      "epoch: 12600, loss: 0.09222354739904404\n",
      "epoch: 12700, loss: 0.09250904619693756\n",
      "epoch: 12800, loss: 0.09068862348794937\n",
      "epoch: 12900, loss: 0.09099406003952026\n",
      "epoch: 13000, loss: 0.08768483996391296\n",
      "epoch: 13100, loss: 0.08751118928194046\n",
      "epoch: 13200, loss: 0.08676356077194214\n",
      "epoch: 13300, loss: 0.0861835777759552\n",
      "epoch: 13400, loss: 0.08434374630451202\n",
      "epoch: 13500, loss: 0.08428177237510681\n",
      "epoch: 13600, loss: 0.08359427005052567\n",
      "epoch: 13700, loss: 0.08274432271718979\n",
      "epoch: 13800, loss: 0.08234252780675888\n",
      "epoch: 13900, loss: 0.0822618380188942\n",
      "epoch: 14000, loss: 0.0815088301897049\n",
      "epoch: 14100, loss: 0.08060110360383987\n",
      "epoch: 14200, loss: 0.08054763078689575\n",
      "epoch: 14300, loss: 0.07951513677835464\n",
      "epoch: 14400, loss: 0.079071544110775\n",
      "epoch: 14500, loss: 0.07883688062429428\n",
      "epoch: 14600, loss: 0.07869929075241089\n",
      "epoch: 14700, loss: 0.07792539894580841\n",
      "epoch: 14800, loss: 0.07766332477331161\n",
      "epoch: 14900, loss: 0.07724741101264954\n",
      "epoch: 15000, loss: 0.07685475796461105\n",
      "epoch: 15100, loss: 0.07645236700773239\n",
      "epoch: 15200, loss: 0.07632418721914291\n",
      "epoch: 15300, loss: 0.0762597844004631\n",
      "epoch: 15400, loss: 0.07570094615221024\n",
      "epoch: 15500, loss: 0.07591607421636581\n",
      "epoch: 15600, loss: 0.07577371597290039\n",
      "epoch: 15700, loss: 0.07516150176525116\n",
      "epoch: 15800, loss: 0.07504936307668686\n",
      "epoch: 15900, loss: 0.07518481463193893\n",
      "epoch: 16000, loss: 0.07414261251688004\n",
      "epoch: 16100, loss: 0.07417891919612885\n",
      "epoch: 16200, loss: 0.07423793524503708\n",
      "epoch: 16300, loss: 0.07364142686128616\n",
      "epoch: 16400, loss: 0.07381656020879745\n",
      "epoch: 16500, loss: 0.07356877624988556\n",
      "epoch: 16600, loss: 0.07332316786050797\n",
      "epoch: 16700, loss: 0.07336217164993286\n",
      "epoch: 16800, loss: 0.07322268187999725\n",
      "epoch: 16900, loss: 0.07266528159379959\n",
      "epoch: 17000, loss: 0.07265444099903107\n",
      "epoch: 17100, loss: 0.07267142087221146\n",
      "epoch: 17200, loss: 0.07255848497152328\n",
      "epoch: 17300, loss: 0.07298370450735092\n",
      "epoch: 17400, loss: 0.07226438820362091\n",
      "epoch: 17500, loss: 0.07252238690853119\n",
      "epoch: 17600, loss: 0.07205545157194138\n",
      "epoch: 17700, loss: 0.0718812644481659\n",
      "epoch: 17800, loss: 0.0718572735786438\n",
      "epoch: 17900, loss: 0.07134633511304855\n",
      "epoch: 18000, loss: 0.07151739299297333\n",
      "epoch: 18100, loss: 0.07125693559646606\n",
      "epoch: 18200, loss: 0.07146559655666351\n",
      "epoch: 18300, loss: 0.07103998214006424\n",
      "epoch: 18400, loss: 0.07109326869249344\n",
      "epoch: 18500, loss: 0.07090307027101517\n",
      "epoch: 18600, loss: 0.07079263776540756\n",
      "epoch: 18700, loss: 0.07080242782831192\n",
      "epoch: 18800, loss: 0.07103051245212555\n",
      "epoch: 18900, loss: 0.07060498744249344\n",
      "epoch: 19000, loss: 0.07051239907741547\n",
      "epoch: 19100, loss: 0.07057295739650726\n",
      "epoch: 19200, loss: 0.07029513269662857\n",
      "epoch: 19300, loss: 0.0703309178352356\n",
      "epoch: 19400, loss: 0.07011719793081284\n",
      "epoch: 19500, loss: 0.07007818669080734\n",
      "epoch: 19600, loss: 0.0696977972984314\n",
      "epoch: 19700, loss: 0.06988348066806793\n",
      "epoch: 19800, loss: 0.06957880407571793\n",
      "epoch: 19900, loss: 0.06938901543617249\n",
      "epoch: 20000, loss: 0.06949435919523239\n",
      "epoch: 20100, loss: 0.06925410777330399\n",
      "epoch: 20200, loss: 0.06941356509923935\n",
      "epoch: 20300, loss: 0.06902944296598434\n",
      "epoch: 20400, loss: 0.06912482529878616\n",
      "epoch: 20500, loss: 0.06892627477645874\n",
      "epoch: 20600, loss: 0.06915370374917984\n",
      "epoch: 20700, loss: 0.06865248829126358\n",
      "epoch: 20800, loss: 0.06842155009508133\n",
      "epoch: 20900, loss: 0.06837005168199539\n",
      "epoch: 21000, loss: 0.0683211088180542\n",
      "epoch: 21100, loss: 0.06806275248527527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21200, loss: 0.06783156096935272\n",
      "epoch: 21300, loss: 0.06736885011196136\n",
      "epoch: 21400, loss: 0.067563995718956\n",
      "epoch: 21500, loss: 0.0672747939825058\n",
      "epoch: 21600, loss: 0.06718931347131729\n",
      "epoch: 21700, loss: 0.0669659674167633\n",
      "epoch: 21800, loss: 0.06686516106128693\n",
      "epoch: 21900, loss: 0.06701202690601349\n",
      "epoch: 22000, loss: 0.06636428833007812\n",
      "epoch: 22100, loss: 0.06603124737739563\n",
      "epoch: 22200, loss: 0.06601136922836304\n",
      "epoch: 22300, loss: 0.06565732508897781\n",
      "epoch: 22400, loss: 0.06536722928285599\n",
      "epoch: 22500, loss: 0.0654047280550003\n",
      "epoch: 22600, loss: 0.0649365559220314\n",
      "epoch: 22700, loss: 0.06472795456647873\n",
      "epoch: 22800, loss: 0.06455153971910477\n",
      "epoch: 22900, loss: 0.063878633081913\n",
      "epoch: 23000, loss: 0.06314877420663834\n",
      "epoch: 23100, loss: 0.06321681290864944\n",
      "epoch: 23200, loss: 0.0629044622182846\n",
      "epoch: 23300, loss: 0.06220986321568489\n",
      "epoch: 23400, loss: 0.0622527040541172\n",
      "epoch: 23500, loss: 0.0621127188205719\n",
      "epoch: 23600, loss: 0.060918863862752914\n",
      "epoch: 23700, loss: 0.06096892058849335\n",
      "epoch: 23800, loss: 0.06040797382593155\n",
      "epoch: 23900, loss: 0.059627555310726166\n",
      "epoch: 24000, loss: 0.05934278294444084\n",
      "epoch: 24100, loss: 0.058617547154426575\n",
      "epoch: 24200, loss: 0.05813662335276604\n",
      "epoch: 24300, loss: 0.05760766193270683\n",
      "epoch: 24400, loss: 0.056882862001657486\n",
      "epoch: 24500, loss: 0.05657712370157242\n",
      "epoch: 24600, loss: 0.055855900049209595\n",
      "epoch: 24700, loss: 0.05522377789020538\n",
      "epoch: 24800, loss: 0.05474796146154404\n",
      "epoch: 24900, loss: 0.053842443972826004\n",
      "epoch: 25000, loss: 0.05300557240843773\n",
      "epoch: 25100, loss: 0.05275573581457138\n",
      "epoch: 25200, loss: 0.052204594016075134\n",
      "epoch: 25300, loss: 0.0515829436480999\n",
      "epoch: 25400, loss: 0.05086788162589073\n",
      "epoch: 25500, loss: 0.05053915083408356\n",
      "epoch: 25600, loss: 0.049928221851587296\n",
      "epoch: 25700, loss: 0.049395374953746796\n",
      "epoch: 25800, loss: 0.04899502545595169\n",
      "epoch: 25900, loss: 0.04812571033835411\n",
      "epoch: 26000, loss: 0.04785050451755524\n",
      "epoch: 26100, loss: 0.04730714485049248\n",
      "epoch: 26200, loss: 0.04690719023346901\n",
      "epoch: 26300, loss: 0.046222902834415436\n",
      "epoch: 26400, loss: 0.04631007835268974\n",
      "epoch: 26500, loss: 0.045530226081609726\n",
      "epoch: 26600, loss: 0.04526913911104202\n",
      "epoch: 26700, loss: 0.044630274176597595\n",
      "epoch: 26800, loss: 0.0443800687789917\n",
      "epoch: 26900, loss: 0.04402565956115723\n",
      "epoch: 27000, loss: 0.043817903846502304\n",
      "epoch: 27100, loss: 0.04345377907156944\n",
      "epoch: 27200, loss: 0.0430990569293499\n",
      "epoch: 27300, loss: 0.042903970927000046\n",
      "epoch: 27400, loss: 0.04250338301062584\n",
      "epoch: 27500, loss: 0.04256262257695198\n",
      "epoch: 27600, loss: 0.04235050082206726\n",
      "epoch: 27700, loss: 0.041858088225126266\n",
      "epoch: 27800, loss: 0.04168142005801201\n",
      "epoch: 27900, loss: 0.04165602847933769\n",
      "epoch: 28000, loss: 0.041209444403648376\n",
      "epoch: 28100, loss: 0.041017938405275345\n",
      "epoch: 28200, loss: 0.04060973972082138\n",
      "epoch: 28300, loss: 0.04038097336888313\n",
      "epoch: 28400, loss: 0.040665511041879654\n",
      "epoch: 28500, loss: 0.040219541639089584\n",
      "epoch: 28600, loss: 0.04022366553544998\n",
      "epoch: 28700, loss: 0.03978113457560539\n",
      "epoch: 28800, loss: 0.039835624396800995\n",
      "epoch: 28900, loss: 0.03962889686226845\n",
      "epoch: 29000, loss: 0.039263661950826645\n",
      "epoch: 29100, loss: 0.03920373320579529\n",
      "epoch: 29200, loss: 0.03893117979168892\n",
      "epoch: 29300, loss: 0.03905143961310387\n",
      "epoch: 29400, loss: 0.03872929885983467\n",
      "epoch: 29500, loss: 0.03862330690026283\n",
      "epoch: 29600, loss: 0.03846028819680214\n",
      "epoch: 29700, loss: 0.03811713308095932\n",
      "epoch: 29800, loss: 0.03810029849410057\n",
      "epoch: 29900, loss: 0.03826833888888359\n",
      "epoch: 30000, loss: 0.037809260189533234\n",
      "epoch: 30100, loss: 0.03776289150118828\n",
      "epoch: 30200, loss: 0.037502728402614594\n",
      "epoch: 30300, loss: 0.037400539964437485\n",
      "epoch: 30400, loss: 0.03725801780819893\n",
      "epoch: 30500, loss: 0.03725438937544823\n",
      "epoch: 30600, loss: 0.03670676052570343\n",
      "epoch: 30700, loss: 0.036936961114406586\n",
      "epoch: 30800, loss: 0.03692253306508064\n",
      "epoch: 30900, loss: 0.03652339428663254\n",
      "epoch: 31000, loss: 0.036537136882543564\n",
      "epoch: 31100, loss: 0.03648866340517998\n",
      "epoch: 31200, loss: 0.03628496453166008\n",
      "epoch: 31300, loss: 0.036180030554533005\n",
      "epoch: 31400, loss: 0.03592962771654129\n",
      "epoch: 31500, loss: 0.03609270602464676\n",
      "epoch: 31600, loss: 0.03590580075979233\n",
      "epoch: 31700, loss: 0.03613591566681862\n",
      "epoch: 31800, loss: 0.035846248269081116\n",
      "epoch: 31900, loss: 0.03538309410214424\n",
      "epoch: 32000, loss: 0.0351545624434948\n",
      "epoch: 32100, loss: 0.03518238663673401\n",
      "epoch: 32200, loss: 0.03495311737060547\n",
      "epoch: 32300, loss: 0.03494158014655113\n",
      "epoch: 32400, loss: 0.03472251817584038\n",
      "epoch: 32500, loss: 0.034703921526670456\n",
      "epoch: 32600, loss: 0.034621406346559525\n",
      "epoch: 32700, loss: 0.034539595246315\n",
      "epoch: 32800, loss: 0.034359343349933624\n",
      "epoch: 32900, loss: 0.0341121144592762\n",
      "epoch: 33000, loss: 0.03416343778371811\n",
      "epoch: 33100, loss: 0.033926598727703094\n",
      "epoch: 33200, loss: 0.03397220000624657\n",
      "epoch: 33300, loss: 0.03366261348128319\n",
      "epoch: 33400, loss: 0.033639419823884964\n",
      "epoch: 33500, loss: 0.03359159827232361\n",
      "epoch: 33600, loss: 0.033514853566884995\n",
      "epoch: 33700, loss: 0.03335975855588913\n",
      "epoch: 33800, loss: 0.03320774808526039\n",
      "epoch: 33900, loss: 0.03313224017620087\n",
      "epoch: 34000, loss: 0.0329834520816803\n",
      "epoch: 34100, loss: 0.0326271653175354\n",
      "epoch: 34200, loss: 0.03281106427311897\n",
      "epoch: 34300, loss: 0.03270295262336731\n",
      "epoch: 34400, loss: 0.032669130712747574\n",
      "epoch: 34500, loss: 0.032316263765096664\n",
      "epoch: 34600, loss: 0.03225788101553917\n",
      "epoch: 34700, loss: 0.03228015825152397\n",
      "epoch: 34800, loss: 0.032281260937452316\n",
      "epoch: 34900, loss: 0.032165687531232834\n",
      "epoch: 35000, loss: 0.03207383677363396\n",
      "epoch: 35100, loss: 0.03188274800777435\n",
      "epoch: 35200, loss: 0.0316460020840168\n",
      "epoch: 35300, loss: 0.031663648784160614\n",
      "epoch: 35400, loss: 0.031541790813207626\n",
      "epoch: 35500, loss: 0.031541913747787476\n",
      "epoch: 35600, loss: 0.03148080036044121\n",
      "epoch: 35700, loss: 0.03150799125432968\n",
      "epoch: 35800, loss: 0.03136484697461128\n",
      "epoch: 35900, loss: 0.03117944486439228\n",
      "epoch: 36000, loss: 0.030914075672626495\n",
      "epoch: 36100, loss: 0.031142733991146088\n",
      "epoch: 36200, loss: 0.03093669004738331\n",
      "epoch: 36300, loss: 0.030889512971043587\n",
      "epoch: 36400, loss: 0.030681036412715912\n",
      "epoch: 36500, loss: 0.030658194795250893\n",
      "epoch: 36600, loss: 0.030638670548796654\n",
      "epoch: 36700, loss: 0.03050954081118107\n",
      "epoch: 36800, loss: 0.03058519773185253\n",
      "epoch: 36900, loss: 0.030419474467635155\n",
      "epoch: 37000, loss: 0.030363934114575386\n",
      "epoch: 37100, loss: 0.030511895194649696\n",
      "epoch: 37200, loss: 0.030157428234815598\n",
      "epoch: 37300, loss: 0.030076678842306137\n",
      "epoch: 37400, loss: 0.029999807476997375\n",
      "epoch: 37500, loss: 0.02986433170735836\n",
      "epoch: 37600, loss: 0.02986972965300083\n",
      "epoch: 37700, loss: 0.029687421396374702\n",
      "epoch: 37800, loss: 0.02971731126308441\n",
      "epoch: 37900, loss: 0.029897810891270638\n",
      "epoch: 38000, loss: 0.029701372608542442\n",
      "epoch: 38100, loss: 0.029523663222789764\n",
      "epoch: 38200, loss: 0.02972070872783661\n",
      "epoch: 38300, loss: 0.029635930433869362\n",
      "epoch: 38400, loss: 0.02969791181385517\n",
      "epoch: 38500, loss: 0.02939917892217636\n",
      "epoch: 38600, loss: 0.02938808873295784\n",
      "epoch: 38700, loss: 0.0294385626912117\n",
      "epoch: 38800, loss: 0.02921009249985218\n",
      "epoch: 38900, loss: 0.029295235872268677\n",
      "epoch: 39000, loss: 0.029248958453536034\n",
      "epoch: 39100, loss: 0.02914615161716938\n",
      "epoch: 39200, loss: 0.02910752035677433\n",
      "epoch: 39300, loss: 0.029046813026070595\n",
      "epoch: 39400, loss: 0.029018472880125046\n",
      "epoch: 39500, loss: 0.028989668935537338\n",
      "epoch: 39600, loss: 0.029095856472849846\n",
      "epoch: 39700, loss: 0.029005886986851692\n",
      "epoch: 39800, loss: 0.028921237215399742\n",
      "epoch: 39900, loss: 0.028937770053744316\n",
      "epoch: 40000, loss: 0.029012100771069527\n",
      "epoch: 40100, loss: 0.02891126461327076\n",
      "epoch: 40200, loss: 0.028716647997498512\n",
      "epoch: 40300, loss: 0.028608081862330437\n",
      "epoch: 40400, loss: 0.028519097715616226\n",
      "epoch: 40500, loss: 0.028533436357975006\n",
      "epoch: 40600, loss: 0.028396975249052048\n",
      "epoch: 40700, loss: 0.028480196371674538\n",
      "epoch: 40800, loss: 0.02852366492152214\n",
      "epoch: 40900, loss: 0.02859848365187645\n",
      "epoch: 41000, loss: 0.028476225212216377\n",
      "epoch: 41100, loss: 0.028606733307242393\n",
      "epoch: 41200, loss: 0.02863217331469059\n",
      "epoch: 41300, loss: 0.028256138786673546\n",
      "epoch: 41400, loss: 0.028633026406168938\n",
      "epoch: 41500, loss: 0.028179533779621124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41600, loss: 0.02844228409230709\n",
      "epoch: 41700, loss: 0.028348887339234352\n",
      "epoch: 41800, loss: 0.02816794253885746\n",
      "epoch: 41900, loss: 0.028217464685440063\n",
      "epoch: 42000, loss: 0.028120433911681175\n",
      "epoch: 42100, loss: 0.028264807537198067\n",
      "epoch: 42200, loss: 0.028165804222226143\n",
      "epoch: 42300, loss: 0.02813138999044895\n",
      "epoch: 42400, loss: 0.027977805584669113\n",
      "epoch: 42500, loss: 0.028132684528827667\n",
      "epoch: 42600, loss: 0.02814204804599285\n",
      "epoch: 42700, loss: 0.02795899473130703\n",
      "epoch: 42800, loss: 0.027983855456113815\n",
      "epoch: 42900, loss: 0.027998581528663635\n",
      "epoch: 43000, loss: 0.02822699211537838\n",
      "epoch: 43100, loss: 0.02795564942061901\n",
      "epoch: 43200, loss: 0.02814740315079689\n",
      "epoch: 43300, loss: 0.027959927916526794\n",
      "epoch: 43400, loss: 0.028044959530234337\n",
      "epoch: 43500, loss: 0.02791566029191017\n",
      "epoch: 43600, loss: 0.02797209471464157\n",
      "epoch: 43700, loss: 0.027946529909968376\n",
      "epoch: 43800, loss: 0.028044044971466064\n",
      "epoch: 43900, loss: 0.027942173182964325\n",
      "epoch: 44000, loss: 0.02779616042971611\n",
      "epoch: 44100, loss: 0.02783486619591713\n",
      "epoch: 44200, loss: 0.027948884293437004\n",
      "epoch: 44300, loss: 0.0277433879673481\n",
      "epoch: 44400, loss: 0.027802808210253716\n",
      "epoch: 44500, loss: 0.02761797420680523\n",
      "epoch: 44600, loss: 0.027811199426651\n",
      "epoch: 44700, loss: 0.027788842096924782\n",
      "epoch: 44800, loss: 0.027639111503958702\n",
      "epoch: 44900, loss: 0.027604201808571815\n",
      "epoch: 45000, loss: 0.027678854763507843\n",
      "epoch: 45100, loss: 0.027529017999768257\n",
      "epoch: 45200, loss: 0.027684492990374565\n",
      "epoch: 45300, loss: 0.027577076107263565\n",
      "epoch: 45400, loss: 0.027764620259404182\n",
      "epoch: 45500, loss: 0.027495166286826134\n",
      "epoch: 45600, loss: 0.027441473677754402\n",
      "epoch: 45700, loss: 0.027729572728276253\n",
      "epoch: 45800, loss: 0.027926502749323845\n",
      "epoch: 45900, loss: 0.027665287256240845\n",
      "epoch: 46000, loss: 0.027547787874937057\n",
      "epoch: 46100, loss: 0.027412470430135727\n",
      "epoch: 46200, loss: 0.027634959667921066\n",
      "epoch: 46300, loss: 0.027650512754917145\n",
      "epoch: 46400, loss: 0.027685316279530525\n",
      "epoch: 46500, loss: 0.027834875509142876\n",
      "epoch: 46600, loss: 0.027621442452073097\n",
      "epoch: 46700, loss: 0.02741057425737381\n",
      "epoch: 46800, loss: 0.027539903298020363\n",
      "epoch: 46900, loss: 0.02770426496863365\n",
      "epoch: 47000, loss: 0.027501743286848068\n",
      "epoch: 47100, loss: 0.027628231793642044\n",
      "epoch: 47200, loss: 0.027708625420928\n",
      "epoch: 47300, loss: 0.02772912010550499\n",
      "epoch: 47400, loss: 0.02753249555826187\n",
      "epoch: 47500, loss: 0.02739078924059868\n",
      "epoch: 47600, loss: 0.02752826362848282\n",
      "epoch: 47700, loss: 0.02744956873357296\n",
      "epoch: 47800, loss: 0.027855204418301582\n",
      "epoch: 47900, loss: 0.02739662304520607\n",
      "epoch: 48000, loss: 0.027487996965646744\n",
      "epoch: 48100, loss: 0.02735435776412487\n",
      "epoch: 48200, loss: 0.027578312903642654\n",
      "epoch: 48300, loss: 0.027416998520493507\n",
      "epoch: 48400, loss: 0.027259740978479385\n",
      "epoch: 48500, loss: 0.02725864015519619\n",
      "epoch: 48600, loss: 0.027401011437177658\n",
      "epoch: 48700, loss: 0.027282027527689934\n",
      "epoch: 48800, loss: 0.0274177398532629\n",
      "epoch: 48900, loss: 0.027226503938436508\n",
      "epoch: 49000, loss: 0.02729029394686222\n",
      "epoch: 49100, loss: 0.027464576065540314\n",
      "epoch: 49200, loss: 0.027292907238006592\n",
      "epoch: 49300, loss: 0.027258170768618584\n",
      "epoch: 49400, loss: 0.027261490002274513\n",
      "epoch: 49500, loss: 0.0273950956761837\n",
      "epoch: 49600, loss: 0.027490319684147835\n",
      "epoch: 49700, loss: 0.027172336354851723\n",
      "epoch: 49800, loss: 0.027141280472278595\n",
      "epoch: 49900, loss: 0.027091948315501213\n",
      "epoch: 50000, loss: 0.02721361815929413\n",
      "epoch: 50100, loss: 0.02722976729273796\n",
      "epoch: 50200, loss: 0.027124028652906418\n",
      "epoch: 50300, loss: 0.027351198717951775\n",
      "epoch: 50400, loss: 0.027421077713370323\n",
      "epoch: 50500, loss: 0.02699568122625351\n",
      "epoch: 50600, loss: 0.027165241539478302\n",
      "epoch: 50700, loss: 0.0271456241607666\n",
      "epoch: 50800, loss: 0.027262350544333458\n",
      "epoch: 50900, loss: 0.02723570540547371\n",
      "epoch: 51000, loss: 0.02731986902654171\n",
      "epoch: 51100, loss: 0.02706976793706417\n",
      "epoch: 51200, loss: 0.027157563716173172\n",
      "epoch: 51300, loss: 0.027414513751864433\n",
      "epoch: 51400, loss: 0.02712419256567955\n",
      "epoch: 51500, loss: 0.027192167937755585\n",
      "epoch: 51600, loss: 0.027193984016776085\n",
      "epoch: 51700, loss: 0.027133412659168243\n",
      "epoch: 51800, loss: 0.027150912210345268\n",
      "epoch: 51900, loss: 0.027187136933207512\n",
      "epoch: 52000, loss: 0.02723638340830803\n",
      "epoch: 52100, loss: 0.02710425667464733\n",
      "epoch: 52200, loss: 0.02687285840511322\n",
      "epoch: 52300, loss: 0.02700166217982769\n",
      "epoch: 52400, loss: 0.027187509462237358\n",
      "epoch: 52500, loss: 0.027362441644072533\n",
      "epoch: 52600, loss: 0.026988493278622627\n",
      "epoch: 52700, loss: 0.0271306112408638\n",
      "epoch: 52800, loss: 0.027269460260868073\n",
      "epoch: 52900, loss: 0.02708853967487812\n",
      "epoch: 53000, loss: 0.02702724002301693\n",
      "epoch: 53100, loss: 0.02703995257616043\n",
      "epoch: 53200, loss: 0.02696838788688183\n",
      "epoch: 53300, loss: 0.027044231072068214\n",
      "epoch: 53400, loss: 0.027322569862008095\n",
      "epoch: 53500, loss: 0.02696814574301243\n",
      "epoch: 53600, loss: 0.02721298113465309\n",
      "epoch: 53700, loss: 0.026989953592419624\n",
      "epoch: 53800, loss: 0.027003245428204536\n",
      "epoch: 53900, loss: 0.027065148577094078\n",
      "epoch: 54000, loss: 0.02712431363761425\n",
      "epoch: 54100, loss: 0.026890190318226814\n",
      "epoch: 54200, loss: 0.02689255401492119\n",
      "epoch: 54300, loss: 0.02696126699447632\n",
      "epoch: 54400, loss: 0.027071142569184303\n",
      "epoch: 54500, loss: 0.027077272534370422\n",
      "epoch: 54600, loss: 0.027092797681689262\n",
      "epoch: 54700, loss: 0.026965897530317307\n",
      "epoch: 54800, loss: 0.027055470272898674\n",
      "epoch: 54900, loss: 0.02702743373811245\n",
      "epoch: 55000, loss: 0.027013182640075684\n",
      "epoch: 55100, loss: 0.027000002562999725\n",
      "epoch: 55200, loss: 0.026972530409693718\n",
      "epoch: 55300, loss: 0.02704513818025589\n",
      "epoch: 55400, loss: 0.026823615655303\n",
      "epoch: 55500, loss: 0.02684473618865013\n",
      "epoch: 55600, loss: 0.02682517282664776\n",
      "epoch: 55700, loss: 0.027015410363674164\n",
      "epoch: 55800, loss: 0.026924585923552513\n",
      "epoch: 55900, loss: 0.027028094977140427\n",
      "epoch: 56000, loss: 0.02688511647284031\n",
      "epoch: 56100, loss: 0.027084076777100563\n",
      "epoch: 56200, loss: 0.027098920196294785\n",
      "epoch: 56300, loss: 0.026941394433379173\n",
      "epoch: 56400, loss: 0.026881840080022812\n",
      "epoch: 56500, loss: 0.026846639811992645\n",
      "epoch: 56600, loss: 0.026875190436840057\n",
      "epoch: 56700, loss: 0.027047978714108467\n",
      "epoch: 56800, loss: 0.02693435363471508\n",
      "epoch: 56900, loss: 0.026709340512752533\n",
      "epoch: 57000, loss: 0.02698550559580326\n",
      "epoch: 57100, loss: 0.0272037535905838\n",
      "epoch: 57200, loss: 0.026854943484067917\n",
      "epoch: 57300, loss: 0.026859160512685776\n",
      "epoch: 57400, loss: 0.027125800028443336\n",
      "epoch: 57500, loss: 0.02682843990623951\n",
      "epoch: 57600, loss: 0.02691734954714775\n",
      "epoch: 57700, loss: 0.0268257986754179\n",
      "epoch: 57800, loss: 0.02695516310632229\n",
      "epoch: 57900, loss: 0.026901718229055405\n",
      "epoch: 58000, loss: 0.02692265994846821\n",
      "epoch: 58100, loss: 0.02676450088620186\n",
      "epoch: 58200, loss: 0.026893002912402153\n",
      "epoch: 58300, loss: 0.026861732825636864\n",
      "epoch: 58400, loss: 0.02689412049949169\n",
      "epoch: 58500, loss: 0.027046335861086845\n",
      "epoch: 58600, loss: 0.02699142135679722\n",
      "epoch: 58700, loss: 0.026957619935274124\n",
      "epoch: 58800, loss: 0.026623524725437164\n",
      "epoch: 58900, loss: 0.026840204373002052\n",
      "epoch: 59000, loss: 0.02693360298871994\n",
      "epoch: 59100, loss: 0.026722734794020653\n",
      "epoch: 59200, loss: 0.0267705786973238\n",
      "epoch: 59300, loss: 0.026913322508335114\n",
      "epoch: 59400, loss: 0.02655457705259323\n",
      "epoch: 59500, loss: 0.02715708315372467\n",
      "epoch: 59600, loss: 0.026937905699014664\n",
      "epoch: 59700, loss: 0.0269599761813879\n",
      "epoch: 59800, loss: 0.026741959154605865\n",
      "epoch: 59900, loss: 0.02664041519165039\n",
      "epoch: 60000, loss: 0.026975471526384354\n",
      "epoch: 60100, loss: 0.02682989276945591\n",
      "epoch: 60200, loss: 0.026869332417845726\n",
      "epoch: 60300, loss: 0.02687523514032364\n",
      "epoch: 60400, loss: 0.02664012834429741\n",
      "epoch: 60500, loss: 0.026930663734674454\n",
      "epoch: 60600, loss: 0.027035240083932877\n",
      "epoch: 60700, loss: 0.026837892830371857\n",
      "epoch: 60800, loss: 0.026878908276557922\n",
      "epoch: 60900, loss: 0.026962362229824066\n",
      "epoch: 61000, loss: 0.026682382449507713\n",
      "epoch: 61100, loss: 0.026754330843687057\n",
      "epoch: 61200, loss: 0.02669602259993553\n",
      "epoch: 61300, loss: 0.02681327611207962\n",
      "epoch: 61400, loss: 0.026870256289839745\n",
      "epoch: 61500, loss: 0.026631740853190422\n",
      "epoch: 61600, loss: 0.02662150375545025\n",
      "epoch: 61700, loss: 0.026706065982580185\n",
      "epoch: 61800, loss: 0.026740126311779022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 61900, loss: 0.02667098306119442\n",
      "epoch: 62000, loss: 0.026447294279932976\n",
      "epoch: 62100, loss: 0.02672390080988407\n",
      "epoch: 62200, loss: 0.026692984625697136\n",
      "epoch: 62300, loss: 0.02659653313457966\n",
      "epoch: 62400, loss: 0.02674417570233345\n",
      "epoch: 62500, loss: 0.026821691542863846\n",
      "epoch: 62600, loss: 0.026895085349678993\n",
      "epoch: 62700, loss: 0.027237899601459503\n",
      "epoch: 62800, loss: 0.027037274092435837\n",
      "epoch: 62900, loss: 0.026589376851916313\n",
      "epoch: 63000, loss: 0.026618776842951775\n",
      "epoch: 63100, loss: 0.02692478522658348\n",
      "epoch: 63200, loss: 0.026853717863559723\n",
      "epoch: 63300, loss: 0.026863250881433487\n",
      "epoch: 63400, loss: 0.026484567672014236\n",
      "epoch: 63500, loss: 0.026811610907316208\n",
      "epoch: 63600, loss: 0.027249246835708618\n",
      "epoch: 63700, loss: 0.02670368179678917\n",
      "epoch: 63800, loss: 0.026435554027557373\n",
      "epoch: 63900, loss: 0.026951422914862633\n",
      "epoch: 64000, loss: 0.026704825460910797\n",
      "epoch: 64100, loss: 0.026533402502536774\n",
      "epoch: 64200, loss: 0.027012605220079422\n",
      "epoch: 64300, loss: 0.02675921842455864\n",
      "epoch: 64400, loss: 0.02643105387687683\n",
      "epoch: 64500, loss: 0.026853442192077637\n",
      "epoch: 64600, loss: 0.026954490691423416\n",
      "epoch: 64700, loss: 0.026460053399205208\n",
      "epoch: 64800, loss: 0.026601094752550125\n",
      "epoch: 64900, loss: 0.026389095932245255\n",
      "epoch: 65000, loss: 0.026543911546468735\n",
      "epoch: 65100, loss: 0.027073871344327927\n",
      "epoch: 65200, loss: 0.026638060808181763\n",
      "epoch: 65300, loss: 0.026634640991687775\n",
      "epoch: 65400, loss: 0.02673187106847763\n",
      "epoch: 65500, loss: 0.026729023084044456\n",
      "epoch: 65600, loss: 0.026461627334356308\n",
      "epoch: 65700, loss: 0.026436924934387207\n",
      "epoch: 65800, loss: 0.026528719812631607\n",
      "epoch: 65900, loss: 0.02661796472966671\n",
      "epoch: 66000, loss: 0.026746338233351707\n",
      "epoch: 66100, loss: 0.026508383452892303\n",
      "epoch: 66200, loss: 0.02651911787688732\n",
      "epoch: 66300, loss: 0.02654552273452282\n",
      "epoch: 66400, loss: 0.02691570483148098\n",
      "epoch: 66500, loss: 0.02659267745912075\n",
      "epoch: 66600, loss: 0.02656945213675499\n",
      "epoch: 66700, loss: 0.02652469091117382\n",
      "epoch: 66800, loss: 0.026566505432128906\n",
      "epoch: 66900, loss: 0.026951873674988747\n",
      "epoch: 67000, loss: 0.026623351499438286\n",
      "epoch: 67100, loss: 0.026612471789121628\n",
      "epoch: 67200, loss: 0.027020348235964775\n",
      "epoch: 67300, loss: 0.026897989213466644\n",
      "epoch: 67400, loss: 0.026492726057767868\n",
      "epoch: 67500, loss: 0.026631953194737434\n",
      "epoch: 67600, loss: 0.026629410684108734\n",
      "epoch: 67700, loss: 0.026411479339003563\n",
      "epoch: 67800, loss: 0.026557931676506996\n",
      "epoch: 67900, loss: 0.026665914803743362\n",
      "epoch: 68000, loss: 0.02636604942381382\n",
      "epoch: 68100, loss: 0.026671793311834335\n",
      "epoch: 68200, loss: 0.02641017735004425\n",
      "epoch: 68300, loss: 0.026683799922466278\n",
      "epoch: 68400, loss: 0.026652658358216286\n",
      "epoch: 68500, loss: 0.026517556980252266\n",
      "epoch: 68600, loss: 0.0266749057918787\n",
      "epoch: 68700, loss: 0.02669788897037506\n",
      "epoch: 68800, loss: 0.02663600631058216\n",
      "epoch: 68900, loss: 0.026544546708464622\n",
      "epoch: 69000, loss: 0.026700299233198166\n",
      "epoch: 69100, loss: 0.026407772675156593\n",
      "epoch: 69200, loss: 0.02667120285332203\n",
      "epoch: 69300, loss: 0.026282474398612976\n",
      "epoch: 69400, loss: 0.02654155157506466\n",
      "epoch: 69500, loss: 0.026672575622797012\n",
      "epoch: 69600, loss: 0.026394637301564217\n",
      "epoch: 69700, loss: 0.02674347534775734\n",
      "epoch: 69800, loss: 0.026623748242855072\n",
      "epoch: 69900, loss: 0.026534322649240494\n",
      "epoch: 70000, loss: 0.02647455222904682\n",
      "epoch: 70100, loss: 0.0267845056951046\n",
      "epoch: 70200, loss: 0.026411641389131546\n",
      "epoch: 70300, loss: 0.026549778878688812\n",
      "epoch: 70400, loss: 0.02638372965157032\n",
      "epoch: 70500, loss: 0.026491615921258926\n",
      "epoch: 70600, loss: 0.026323724538087845\n",
      "epoch: 70700, loss: 0.02658609114587307\n",
      "epoch: 70800, loss: 0.026576807722449303\n",
      "epoch: 70900, loss: 0.026433132588863373\n",
      "epoch: 71000, loss: 0.02644944377243519\n",
      "epoch: 71100, loss: 0.026317622512578964\n",
      "epoch: 71200, loss: 0.026390790939331055\n",
      "epoch: 71300, loss: 0.026330674067139626\n",
      "epoch: 71400, loss: 0.0267427209764719\n",
      "epoch: 71500, loss: 0.026418279856443405\n",
      "epoch: 71600, loss: 0.026485394686460495\n",
      "epoch: 71700, loss: 0.02665918506681919\n",
      "epoch: 71800, loss: 0.026542136445641518\n",
      "epoch: 71900, loss: 0.026865286752581596\n",
      "epoch: 72000, loss: 0.02647562511265278\n",
      "epoch: 72100, loss: 0.026360824704170227\n",
      "epoch: 72200, loss: 0.026713158935308456\n",
      "epoch: 72300, loss: 0.026531511917710304\n",
      "epoch: 72400, loss: 0.026478484272956848\n",
      "epoch: 72500, loss: 0.026602331548929214\n",
      "epoch: 72600, loss: 0.026457644999027252\n",
      "epoch: 72700, loss: 0.02656988985836506\n",
      "epoch: 72800, loss: 0.026395056396722794\n",
      "epoch: 72900, loss: 0.026517966762185097\n",
      "epoch: 73000, loss: 0.0262873787432909\n",
      "epoch: 73100, loss: 0.026416001841425896\n",
      "epoch: 73200, loss: 0.026420973241329193\n",
      "epoch: 73300, loss: 0.02627849392592907\n",
      "epoch: 73400, loss: 0.02661246992647648\n",
      "epoch: 73500, loss: 0.026324285194277763\n",
      "epoch: 73600, loss: 0.02635810151696205\n",
      "epoch: 73700, loss: 0.02645852044224739\n",
      "epoch: 73800, loss: 0.026234764605760574\n",
      "epoch: 73900, loss: 0.026556948199868202\n",
      "epoch: 74000, loss: 0.026504261419177055\n",
      "epoch: 74100, loss: 0.02663242444396019\n",
      "epoch: 74200, loss: 0.02618350461125374\n",
      "epoch: 74300, loss: 0.026305604726076126\n",
      "epoch: 74400, loss: 0.026487944647669792\n",
      "epoch: 74500, loss: 0.026176495477557182\n",
      "epoch: 74600, loss: 0.026484718546271324\n",
      "epoch: 74700, loss: 0.026298297569155693\n",
      "epoch: 74800, loss: 0.026335272938013077\n",
      "epoch: 74900, loss: 0.02676594816148281\n",
      "epoch: 75000, loss: 0.02644502744078636\n",
      "epoch: 75100, loss: 0.026503944769501686\n",
      "epoch: 75200, loss: 0.026288805529475212\n",
      "epoch: 75300, loss: 0.026602229103446007\n",
      "epoch: 75400, loss: 0.026179615408182144\n",
      "epoch: 75500, loss: 0.026296785101294518\n",
      "epoch: 75600, loss: 0.0264310110360384\n",
      "epoch: 75700, loss: 0.026555191725492477\n",
      "epoch: 75800, loss: 0.026557505130767822\n",
      "epoch: 75900, loss: 0.02647452987730503\n",
      "epoch: 76000, loss: 0.02642343007028103\n",
      "epoch: 76100, loss: 0.026234839111566544\n",
      "epoch: 76200, loss: 0.026131359860301018\n",
      "epoch: 76300, loss: 0.026244623586535454\n",
      "epoch: 76400, loss: 0.02609807439148426\n",
      "epoch: 76500, loss: 0.026333339512348175\n",
      "epoch: 76600, loss: 0.026525821536779404\n",
      "epoch: 76700, loss: 0.02655598893761635\n",
      "epoch: 76800, loss: 0.026319462805986404\n",
      "epoch: 76900, loss: 0.026245716959238052\n",
      "epoch: 77000, loss: 0.02641717530786991\n",
      "epoch: 77100, loss: 0.026271644979715347\n",
      "epoch: 77200, loss: 0.026034530252218246\n",
      "epoch: 77300, loss: 0.026320209726691246\n",
      "epoch: 77400, loss: 0.026186395436525345\n",
      "epoch: 77500, loss: 0.026262985542416573\n",
      "epoch: 77600, loss: 0.026230674237012863\n",
      "epoch: 77700, loss: 0.026114827021956444\n",
      "epoch: 77800, loss: 0.02632146328687668\n",
      "epoch: 77900, loss: 0.026360465213656425\n",
      "epoch: 78000, loss: 0.02615828439593315\n",
      "epoch: 78100, loss: 0.026526575908064842\n",
      "epoch: 78200, loss: 0.026312021538615227\n",
      "epoch: 78300, loss: 0.026535268872976303\n",
      "epoch: 78400, loss: 0.026298684999346733\n",
      "epoch: 78500, loss: 0.026165805757045746\n",
      "epoch: 78600, loss: 0.026419591158628464\n",
      "epoch: 78700, loss: 0.02591639757156372\n",
      "epoch: 78800, loss: 0.0261244960129261\n",
      "epoch: 78900, loss: 0.026076406240463257\n",
      "epoch: 79000, loss: 0.02672743611037731\n",
      "epoch: 79100, loss: 0.026077311486005783\n",
      "epoch: 79200, loss: 0.026242762804031372\n",
      "epoch: 79300, loss: 0.026505442336201668\n",
      "epoch: 79400, loss: 0.026188714429736137\n",
      "epoch: 79500, loss: 0.026031076908111572\n",
      "epoch: 79600, loss: 0.026181524619460106\n",
      "epoch: 79700, loss: 0.02634993940591812\n",
      "epoch: 79800, loss: 0.02629959024488926\n",
      "epoch: 79900, loss: 0.025737818330526352\n",
      "epoch: 80000, loss: 0.02620880864560604\n",
      "epoch: 80100, loss: 0.026386523619294167\n",
      "epoch: 80200, loss: 0.026150967925786972\n",
      "epoch: 80300, loss: 0.026306403800845146\n",
      "epoch: 80400, loss: 0.026617377996444702\n",
      "epoch: 80500, loss: 0.026281138882040977\n",
      "epoch: 80600, loss: 0.026068106293678284\n",
      "epoch: 80700, loss: 0.026150284335017204\n",
      "epoch: 80800, loss: 0.02661214955151081\n",
      "epoch: 80900, loss: 0.026353025808930397\n",
      "epoch: 81000, loss: 0.026275185868144035\n",
      "epoch: 81100, loss: 0.026293715462088585\n",
      "epoch: 81200, loss: 0.026167646050453186\n",
      "epoch: 81300, loss: 0.026692064478993416\n",
      "epoch: 81400, loss: 0.026277871802449226\n",
      "epoch: 81500, loss: 0.026237957179546356\n",
      "epoch: 81600, loss: 0.02612617239356041\n",
      "epoch: 81700, loss: 0.02634899504482746\n",
      "epoch: 81800, loss: 0.02631976082921028\n",
      "epoch: 81900, loss: 0.026275310665369034\n",
      "epoch: 82000, loss: 0.026216203346848488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 82100, loss: 0.02634407766163349\n",
      "epoch: 82200, loss: 0.026389967650175095\n",
      "epoch: 82300, loss: 0.026433920487761497\n",
      "epoch: 82400, loss: 0.026147620752453804\n",
      "epoch: 82500, loss: 0.026148224249482155\n",
      "epoch: 82600, loss: 0.02615172043442726\n",
      "epoch: 82700, loss: 0.026178134605288506\n",
      "epoch: 82800, loss: 0.026056088507175446\n",
      "epoch: 82900, loss: 0.02637457847595215\n",
      "epoch: 83000, loss: 0.026187947019934654\n",
      "epoch: 83100, loss: 0.026124171912670135\n",
      "epoch: 83200, loss: 0.026237808167934418\n",
      "epoch: 83300, loss: 0.026216696947813034\n",
      "epoch: 83400, loss: 0.026526469737291336\n",
      "epoch: 83500, loss: 0.026118941605091095\n",
      "epoch: 83600, loss: 0.025985104963183403\n",
      "epoch: 83700, loss: 0.02625962346792221\n",
      "epoch: 83800, loss: 0.026196878403425217\n",
      "epoch: 83900, loss: 0.026420727372169495\n",
      "epoch: 84000, loss: 0.026052460074424744\n",
      "epoch: 84100, loss: 0.026229022070765495\n",
      "epoch: 84200, loss: 0.026026099920272827\n",
      "epoch: 84300, loss: 0.026179909706115723\n",
      "epoch: 84400, loss: 0.026186442002654076\n",
      "epoch: 84500, loss: 0.026057109236717224\n",
      "epoch: 84600, loss: 0.026094669476151466\n",
      "epoch: 84700, loss: 0.026145393028855324\n",
      "epoch: 84800, loss: 0.02649073861539364\n",
      "epoch: 84900, loss: 0.02614631876349449\n",
      "epoch: 85000, loss: 0.026125406846404076\n",
      "epoch: 85100, loss: 0.026281924918293953\n",
      "epoch: 85200, loss: 0.026180410757660866\n",
      "epoch: 85300, loss: 0.026339178904891014\n",
      "epoch: 85400, loss: 0.02618754841387272\n",
      "epoch: 85500, loss: 0.026015320792794228\n",
      "epoch: 85600, loss: 0.026324857026338577\n",
      "epoch: 85700, loss: 0.026012031361460686\n",
      "epoch: 85800, loss: 0.026237590238451958\n",
      "epoch: 85900, loss: 0.02599445730447769\n",
      "epoch: 86000, loss: 0.026152964681386948\n",
      "epoch: 86100, loss: 0.02621915377676487\n",
      "epoch: 86200, loss: 0.026154886931180954\n",
      "epoch: 86300, loss: 0.026043221354484558\n",
      "epoch: 86400, loss: 0.02601705491542816\n",
      "epoch: 86500, loss: 0.025901732966303825\n",
      "epoch: 86600, loss: 0.026325765997171402\n",
      "epoch: 86700, loss: 0.025970255956053734\n",
      "epoch: 86800, loss: 0.025756187736988068\n",
      "epoch: 86900, loss: 0.026017749682068825\n",
      "epoch: 87000, loss: 0.026348240673542023\n",
      "epoch: 87100, loss: 0.02632567100226879\n",
      "epoch: 87200, loss: 0.025991166010499\n",
      "epoch: 87300, loss: 0.026290163397789\n",
      "epoch: 87400, loss: 0.026136985048651695\n",
      "epoch: 87500, loss: 0.02576780505478382\n",
      "epoch: 87600, loss: 0.026331301778554916\n",
      "epoch: 87700, loss: 0.02620495855808258\n",
      "epoch: 87800, loss: 0.026122722774744034\n",
      "epoch: 87900, loss: 0.026130570098757744\n",
      "epoch: 88000, loss: 0.0262028556317091\n",
      "epoch: 88100, loss: 0.026083853095769882\n",
      "epoch: 88200, loss: 0.026266569271683693\n",
      "epoch: 88300, loss: 0.026411183178424835\n",
      "epoch: 88400, loss: 0.025966722518205643\n",
      "epoch: 88500, loss: 0.026046378538012505\n",
      "epoch: 88600, loss: 0.026000041514635086\n",
      "epoch: 88700, loss: 0.026323700323700905\n",
      "epoch: 88800, loss: 0.026065580546855927\n",
      "epoch: 88900, loss: 0.026422303169965744\n",
      "epoch: 89000, loss: 0.026233838871121407\n",
      "epoch: 89100, loss: 0.026355646550655365\n",
      "epoch: 89200, loss: 0.026012560352683067\n",
      "epoch: 89300, loss: 0.02616795338690281\n",
      "epoch: 89400, loss: 0.02587314136326313\n",
      "epoch: 89500, loss: 0.026300568133592606\n",
      "epoch: 89600, loss: 0.025964155793190002\n",
      "epoch: 89700, loss: 0.026168297976255417\n",
      "epoch: 89800, loss: 0.026331447064876556\n",
      "epoch: 89900, loss: 0.026146000251173973\n",
      "epoch: 90000, loss: 0.02608427219092846\n",
      "epoch: 90100, loss: 0.02616623230278492\n",
      "epoch: 90200, loss: 0.02610105089843273\n",
      "epoch: 90300, loss: 0.025782469660043716\n",
      "epoch: 90400, loss: 0.02605634555220604\n",
      "epoch: 90500, loss: 0.02591501921415329\n",
      "epoch: 90600, loss: 0.02624303288757801\n",
      "epoch: 90700, loss: 0.026064440608024597\n",
      "epoch: 90800, loss: 0.026158222928643227\n",
      "epoch: 90900, loss: 0.026037532836198807\n",
      "epoch: 91000, loss: 0.02591385692358017\n",
      "epoch: 91100, loss: 0.025886638090014458\n",
      "epoch: 91200, loss: 0.025909801945090294\n",
      "epoch: 91300, loss: 0.02594997175037861\n",
      "epoch: 91400, loss: 0.0262149665504694\n",
      "epoch: 91500, loss: 0.026136675849556923\n",
      "epoch: 91600, loss: 0.02624848671257496\n",
      "epoch: 91700, loss: 0.026151247322559357\n",
      "epoch: 91800, loss: 0.02615303546190262\n",
      "epoch: 91900, loss: 0.026086093857884407\n",
      "epoch: 92000, loss: 0.025952523574233055\n",
      "epoch: 92100, loss: 0.026048684492707253\n",
      "epoch: 92200, loss: 0.02620045468211174\n",
      "epoch: 92300, loss: 0.02619943395256996\n",
      "epoch: 92400, loss: 0.02615421451628208\n",
      "epoch: 92500, loss: 0.0260801799595356\n",
      "epoch: 92600, loss: 0.026288943365216255\n",
      "epoch: 92700, loss: 0.02600337564945221\n",
      "epoch: 92800, loss: 0.02579818293452263\n",
      "epoch: 92900, loss: 0.0261155404150486\n",
      "epoch: 93000, loss: 0.025971975177526474\n",
      "epoch: 93100, loss: 0.025915561243891716\n",
      "epoch: 93200, loss: 0.02623695321381092\n",
      "epoch: 93300, loss: 0.02610647678375244\n",
      "epoch: 93400, loss: 0.026008637621998787\n",
      "epoch: 93500, loss: 0.02578474022448063\n",
      "epoch: 93600, loss: 0.026061294600367546\n",
      "epoch: 93700, loss: 0.02576812170445919\n",
      "epoch: 93800, loss: 0.025963298976421356\n",
      "epoch: 93900, loss: 0.026124712079763412\n",
      "epoch: 94000, loss: 0.025797851383686066\n",
      "epoch: 94100, loss: 0.026410283520817757\n",
      "epoch: 94200, loss: 0.026070330291986465\n",
      "epoch: 94300, loss: 0.02591882087290287\n",
      "epoch: 94400, loss: 0.026126259937882423\n",
      "epoch: 94500, loss: 0.02575453370809555\n",
      "epoch: 94600, loss: 0.02594480849802494\n",
      "epoch: 94700, loss: 0.02620585821568966\n",
      "epoch: 94800, loss: 0.02588788792490959\n",
      "epoch: 94900, loss: 0.026084354147315025\n",
      "epoch: 95000, loss: 0.02594236098229885\n",
      "epoch: 95100, loss: 0.02597144991159439\n",
      "epoch: 95200, loss: 0.0259080957621336\n",
      "epoch: 95300, loss: 0.025457052513957024\n",
      "epoch: 95400, loss: 0.025830281898379326\n",
      "epoch: 95500, loss: 0.02578900195658207\n",
      "epoch: 95600, loss: 0.025945940986275673\n",
      "epoch: 95700, loss: 0.025908246636390686\n",
      "epoch: 95800, loss: 0.0260627381503582\n",
      "epoch: 95900, loss: 0.02623719349503517\n",
      "epoch: 96000, loss: 0.025847259908914566\n",
      "epoch: 96100, loss: 0.025976024568080902\n",
      "epoch: 96200, loss: 0.026224026456475258\n",
      "epoch: 96300, loss: 0.026140237227082253\n",
      "epoch: 96400, loss: 0.02592187002301216\n",
      "epoch: 96500, loss: 0.0261817816644907\n",
      "epoch: 96600, loss: 0.025750741362571716\n",
      "epoch: 96700, loss: 0.025644296780228615\n",
      "epoch: 96800, loss: 0.025669775903224945\n",
      "epoch: 96900, loss: 0.026018831878900528\n",
      "epoch: 97000, loss: 0.026038192212581635\n",
      "epoch: 97100, loss: 0.02605590783059597\n",
      "epoch: 97200, loss: 0.025965223088860512\n",
      "epoch: 97300, loss: 0.026097815483808517\n",
      "epoch: 97400, loss: 0.02634148672223091\n",
      "epoch: 97500, loss: 0.025677137076854706\n",
      "epoch: 97600, loss: 0.025771591812372208\n",
      "epoch: 97700, loss: 0.02592593804001808\n",
      "epoch: 97800, loss: 0.02593608945608139\n",
      "epoch: 97900, loss: 0.026067357510328293\n",
      "epoch: 98000, loss: 0.026306722313165665\n",
      "epoch: 98100, loss: 0.02580311708152294\n",
      "epoch: 98200, loss: 0.02596525475382805\n",
      "epoch: 98300, loss: 0.025882044807076454\n",
      "epoch: 98400, loss: 0.026047848165035248\n",
      "epoch: 98500, loss: 0.0261992197483778\n",
      "epoch: 98600, loss: 0.026254061609506607\n",
      "epoch: 98700, loss: 0.026010330766439438\n",
      "epoch: 98800, loss: 0.025616316124796867\n",
      "epoch: 98900, loss: 0.02611144445836544\n",
      "epoch: 99000, loss: 0.02579110488295555\n",
      "epoch: 99100, loss: 0.025807848200201988\n",
      "epoch: 99200, loss: 0.02605714648962021\n",
      "epoch: 99300, loss: 0.0260534156113863\n",
      "epoch: 99400, loss: 0.02601347491145134\n",
      "epoch: 99500, loss: 0.02581476792693138\n",
      "epoch: 99600, loss: 0.025930270552635193\n",
      "epoch: 99700, loss: 0.02598833478987217\n",
      "epoch: 99800, loss: 0.025960534811019897\n",
      "epoch: 99900, loss: 0.025544917210936546\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# train\n",
    "updater = DynamicsParamsOptimizer(state_dim, action_dim, param_dim, latent_dim, switch_dim, model_save_path)\n",
    "updater.train(data_s, data_a, data_param, data_s_, epoch=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "def display_points(predict_x,true_x):\n",
    "#     plt.xlim(0,5)\n",
    "#     plt.ylim(-3,1)\n",
    "    predict_x = np.array(predict_x)\n",
    "    true_x = np.array(true_x)\n",
    "#     colors = get_cmap(predict_x.shape[0])\n",
    "    \n",
    "    for i, (x, x_) in enumerate(zip(predict_x, true_x)):\n",
    "        c=numpy.random.rand(3,)\n",
    "        plt.plot(*x,\"*\", c=c, markersize=10)\n",
    "        plt.plot(*x_,\"^\", c=c, markersize=8)  \n",
    "    plt.savefig('sld.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in dest data:  10\n",
      "(275717, 11) (275717, 1) (5,) (275717, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test data\n",
    "test_data_path = path+'/data/dynamics_data/'+env_name+'/test_dynamics.npy'\n",
    "test_data = np.load(test_data_path, allow_pickle=True)\n",
    "print('number of samples in dest data: ', len(test_data))\n",
    "idx=5  # index of sample to test: 0-10\n",
    "test_s = np.array(test_data[idx]['sa'])[:, :-1]\n",
    "test_a = np.array(test_data[idx]['sa'])[:, -1:]\n",
    "test_param = np.array(test_data[idx]['params'])\n",
    "test_s_ = np.array(test_data[idx]['s_'])\n",
    "print(test_s.shape, test_a.shape, test_param.shape, test_s_.shape)\n",
    "\n",
    "# load model\n",
    "updater = DynamicsParamsOptimizer(state_dim, action_dim, param_dim, latent_dim, switch_dim, model_save_path)\n",
    "updater.model.load_state_dict(torch.load(model_save_path+'model', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 11]) torch.Size([10, 11, 2]) torch.Size([10, 2])\n",
      "tensor([[ 2.8067, -1.7656],\n",
      "        [ 2.3687, -1.2460],\n",
      "        [ 2.6240, -1.4660],\n",
      "        [ 2.6398, -1.4809],\n",
      "        [ 2.8436, -1.8056],\n",
      "        [ 1.4809,  0.1059],\n",
      "        [ 2.4902, -1.3275],\n",
      "        [ 2.7514, -1.6052],\n",
      "        [ 2.9631, -1.8194],\n",
      "        [ 2.4582, -1.3334]], device='cuda:0', grad_fn=<ViewBackward>) [ 2.5426657 -1.3743666] [[ 2.651606  -1.5125511]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "partial = 10\n",
    "alpha = updater.model.get_latent_code(test_s[:partial], test_a[:partial], test_s_[:partial])\n",
    "average_alpha = alpha.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "# compare with encoded value\n",
    "alpha_ = (torch.Tensor([test_param]).to(device)@updater.model.E).detach().cpu().numpy()\n",
    "print(alpha, average_alpha, alpha_)\n",
    "\n",
    "display_points([average_alpha], alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in dest data:  10\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "test_data_path = path+'/data/dynamics_data/'+env_name+'/test_dynamics.npy'\n",
    "test_data = np.load(test_data_path, allow_pickle=True)\n",
    "print('number of samples in dest data: ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189988, 11) (189988, 1) (5,) (189988, 11)\n",
      "torch.Size([10, 2, 11]) torch.Size([10, 11, 2]) torch.Size([10, 2])\n",
      "[ 2.6493506 -1.5472585] [ 2.617456  -1.4695983]\n",
      "(117608, 11) (117608, 1) (5,) (117608, 11)\n",
      "torch.Size([10, 2, 11]) torch.Size([10, 11, 2]) torch.Size([10, 2])\n",
      "[ 3.679095  -2.8840253] [ 3.420619 -2.491965]\n",
      "(288323, 11) (288323, 1) (5,) (288323, 11)\n",
      "torch.Size([10, 2, 11]) torch.Size([10, 11, 2]) torch.Size([10, 2])\n",
      "[ 2.5891635 -1.4313912] [ 2.8361316 -1.6551087]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_alpha, data_alpha_ = [], []\n",
    "\n",
    "for idx in range(3):\n",
    "#     idx=5  # index of sample to test: 0-10\n",
    "    test_s = np.array(test_data[idx]['sa'])[:, :-1]\n",
    "    test_a = np.array(test_data[idx]['sa'])[:, -1:]\n",
    "    test_param = np.array(test_data[idx]['params'])\n",
    "    test_s_ = np.array(test_data[idx]['s_'])\n",
    "    print(test_s.shape, test_a.shape, test_param.shape, test_s_.shape)\n",
    "\n",
    "    # load model\n",
    "    updater = DynamicsParamsOptimizer(state_dim, action_dim, param_dim, latent_dim, switch_dim, model_save_path)\n",
    "    updater.model.load_state_dict(torch.load(model_save_path+'model', map_location=device))\n",
    "\n",
    "    partial = 10\n",
    "    alpha = updater.model.get_latent_code(test_s[:partial], test_a[:partial], test_s_[:partial])\n",
    "    average_alpha = alpha.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    # compare with encoded value\n",
    "    alpha_ = (torch.Tensor([test_param]).to(device)@updater.model.E)[0].detach().cpu().numpy()\n",
    "    print(average_alpha, alpha_)\n",
    "    data_alpha.append(average_alpha)\n",
    "    data_alpha_.append(alpha_)\n",
    "\n",
    "display_points(data_alpha, data_alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 11]) torch.Size([20, 11, 2]) torch.Size([20, 2])\n",
      "tensor([[ 2.2643, -1.8454],\n",
      "        [ 2.8811, -2.2793],\n",
      "        [ 3.5958, -2.9697],\n",
      "        [ 1.7742, -1.4963],\n",
      "        [ 2.4365, -2.0887],\n",
      "        [ 2.2297, -1.8889],\n",
      "        [ 1.8014, -1.4508],\n",
      "        [ 2.7828, -2.4257],\n",
      "        [ 3.4125, -2.8278],\n",
      "        [ 1.7107, -1.3469],\n",
      "        [ 2.0031, -1.6024],\n",
      "        [ 2.5392, -2.0994],\n",
      "        [ 3.7406, -3.1136],\n",
      "        [ 1.4040, -1.1620],\n",
      "        [ 2.3428, -1.9775],\n",
      "        [ 4.2024, -3.5203],\n",
      "        [ 1.4234, -1.1618],\n",
      "        [ 1.6157, -1.2830],\n",
      "        [ 1.8207, -1.4661],\n",
      "        [ 2.3034, -1.9059]], grad_fn=<ViewBackward>) tensor([[ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# test on train data\n",
    "test_size = 20\n",
    "test_s = data_s[:test_size]\n",
    "test_a = data_a[:test_size]\n",
    "test_param = data_param[:test_size]\n",
    "test_s_ = data_s_[:test_size]\n",
    "\n",
    "alpha = updater.model.get_latent_code(test_s, test_a, test_s_)\n",
    "\n",
    "# compare with encoded value\n",
    "alpha_ = torch.Tensor(test_param)@updater.model.E\n",
    "print(alpha, alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0034d21db9019aa9fe47b3d79d474777c42bec03792ede4a8b556d1ed67f2d91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
