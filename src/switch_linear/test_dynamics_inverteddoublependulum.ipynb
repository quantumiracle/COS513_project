{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/research/COS513_project/src\n",
      "Error: encoder not found!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(),\"..\"))\n",
    "print(path)\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from dynamics_predict.dynamics_networks import DynamicsNetwork, DynamicsParamsOptimizer, EncoderDynamicsNetwork, EncoderDecoderDynamicsNetwork, VAEDynamicsNetwork\n",
    "from rl.policy_networks import DPG_PolicyNetwork\n",
    "from utils.load_params import load_params\n",
    "from utils.common_func import rand_params\n",
    "from dynamics_predict.defaults import DYNAMICS_PARAMS, HYPER_PARAMS\n",
    "from environment import envs\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter dimension:  5\n"
     ]
    }
   ],
   "source": [
    "env_name = 'inverteddoublependulum'\n",
    "data_path = path+'/data/dynamics_data/'+env_name+'/dynamics.npy'\n",
    "param_dim = len(DYNAMICS_PARAMS[env_name+'dynamics'])\n",
    "print('parameter dimension: ', param_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in dest data:  3549\n",
      "(3549, 11) (3549, 1) (3549, 5) (3549, 11)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(data_path, allow_pickle=True)\n",
    "print('number of samples in dest data: ', len(train_data))\n",
    "# split data\n",
    "data_s, data_a, data_param, data_s_ = [], [], [], []\n",
    "for d in train_data:\n",
    "    [s,a,param], s_ = d\n",
    "    data_s.append(s)\n",
    "    data_a.append(a)\n",
    "    data_param.append(param)\n",
    "    data_s_.append(s_)\n",
    "\n",
    "data_s = np.array(data_s)\n",
    "data_a = np.array(data_a)\n",
    "data_param = np.array(data_param)\n",
    "data_s_ = np.array(data_s_)\n",
    "\n",
    "print(data_s.shape, data_a.shape, data_param.shape, data_s_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch Linear Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DynamicsParamsOptimizer():\n",
    "    \"\"\" \n",
    "    Dynamics parameters optimization model (gradient-based) based on a trained \n",
    "    forward dynamics prediction network: (s, a, learnable_params) -> s_ with real-world data. \n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, param_dim, latent_dim, switch_dim, model_save_path):\n",
    "        self.model = SLDynamicsNetwork(state_dim, action_dim, param_dim, latent_dim, switch_dim)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.model_save_path = model_save_path\n",
    "\n",
    "    def train(self, s, a, theta, s_, epoch):\n",
    "        \"\"\" s,a concat with param (learnable) -> s_ \"\"\"\n",
    "        if not isinstance(s_, torch.Tensor):\n",
    "            s_ = torch.Tensor(s_)\n",
    "\n",
    "        for ep in range(epoch):\n",
    "            s_pred = self.model.forward(s, a, theta)\n",
    "            self.model.optimizer.zero_grad()\n",
    "            loss = self.criterion(s_pred, s_)\n",
    "            loss.backward()\n",
    "            self.model.optimizer.step()\n",
    "            if ep%100==0:\n",
    "                print('epoch: {}, loss: {}'.format(ep, loss.item()))\n",
    "                torch.save(self.model.state_dict(), self.model_save_path+'model')\n",
    "            \n",
    "\n",
    "class SLDynamicsNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, param_dim, latent_dim, switch_dim, lr=1e-4):\n",
    "        super(SLDynamicsNetwork, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.A = nn.Parameter(torch.rand((switch_dim, latent_dim, state_dim, state_dim)), requires_grad=True)\n",
    "        self.B = nn.Parameter(torch.rand((switch_dim, latent_dim, state_dim, action_dim)), requires_grad=True)\n",
    "        self.E = nn.Parameter(torch.rand((param_dim, latent_dim)), requires_grad=True)\n",
    "        self.switch_logits = nn.Sequential(\n",
    "            nn.Linear(state_dim, switch_dim, bias=False)  # only weight matrix, no bias\n",
    "        )\n",
    "        # print(dict(self.named_parameters()))\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def gaussian_noise(self, shape, scale):\n",
    "        normal = Normal(0, 1)\n",
    "        epsilon = scale * normal.sample(shape) \n",
    "        return epsilon\n",
    "\n",
    "    def get_switch_var(self, s):\n",
    "        logits_ = self.switch_logits(s)\n",
    "        switch_var = F.gumbel_softmax(logits_, tau=1, hard=True)  # if hard, return one-hot\n",
    "        return switch_var\n",
    "\n",
    "    def get_s_before_encode(self, s, a):\n",
    "        switch_var = self.get_switch_var(s)\n",
    "        A_w = torch.einsum('ab,bcde->acde', switch_var, self.A) # chosen by the switch variable; shape (#batch, #latent, #state, #state)\n",
    "        B_w = torch.einsum('ab,bcde->acde', switch_var, self.B) # chosen by the switch variable; shape (#batch, #latent, #state, #action)\n",
    "        s_before_encode = torch.einsum('abcd,ad->abc', A_w, s) + torch.einsum('abcd,ad->abc', B_w, a)  # shape (#batch, #latent, #state)\n",
    "        return s_before_encode\n",
    "\n",
    "    def forward(self, s, a, theta):\n",
    "        if not isinstance(s, torch.Tensor):\n",
    "            s = torch.Tensor(s)\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.Tensor(a)\n",
    "        if not isinstance(theta, torch.Tensor):\n",
    "            theta = torch.Tensor(theta)\n",
    "        batch_size = s.shape[0]\n",
    "\n",
    "        s_before_encode = self.get_s_before_encode(s, a)\n",
    "        s_before_noise = torch.einsum('ab,abc->ac', theta@self.E, s_before_encode)  # shape (#batch, #state)\n",
    "        noise = self.gaussian_noise(shape=(batch_size, self.state_dim), scale=0.)\n",
    "        s_ = s_before_noise + noise\n",
    "\n",
    "        return s_\n",
    "\n",
    "    def get_latent_code(self, s, a, s_):\n",
    "        if not isinstance(s, torch.Tensor):\n",
    "            s = torch.Tensor(s)\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.Tensor(a)        \n",
    "        if not isinstance(s_, torch.Tensor):\n",
    "            s_ = torch.Tensor(s_)     \n",
    "\n",
    "        s_before_encode = self.get_s_before_encode(s, a)\n",
    "        inv_s = torch.linalg.pinv(s_before_encode)  # pseudo-inverse; shape (#batch, #state, #latent)\n",
    "        alpha = torch.einsum('ab,abc->ac', s_, inv_s)\n",
    "        print(s_before_encode.shape, inv_s.shape, alpha.shape)\n",
    "\n",
    "        return alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 46.492610931396484\n",
      "epoch: 100, loss: 40.28472137451172\n",
      "epoch: 200, loss: 35.36576461791992\n",
      "epoch: 300, loss: 30.831071853637695\n",
      "epoch: 400, loss: 26.42263412475586\n",
      "epoch: 500, loss: 23.265213012695312\n",
      "epoch: 600, loss: 20.834135055541992\n",
      "epoch: 700, loss: 18.181493759155273\n",
      "epoch: 800, loss: 16.19983673095703\n",
      "epoch: 900, loss: 14.607651710510254\n",
      "epoch: 1000, loss: 12.863548278808594\n",
      "epoch: 1100, loss: 11.402203559875488\n",
      "epoch: 1200, loss: 9.925081253051758\n",
      "epoch: 1300, loss: 8.70793342590332\n",
      "epoch: 1400, loss: 7.821108341217041\n",
      "epoch: 1500, loss: 7.195626258850098\n",
      "epoch: 1600, loss: 6.04440975189209\n",
      "epoch: 1700, loss: 5.449192523956299\n",
      "epoch: 1800, loss: 4.671431064605713\n",
      "epoch: 1900, loss: 4.253304958343506\n",
      "epoch: 2000, loss: 3.644782304763794\n",
      "epoch: 2100, loss: 3.162264108657837\n",
      "epoch: 2200, loss: 2.866579294204712\n",
      "epoch: 2300, loss: 2.458381414413452\n",
      "epoch: 2400, loss: 2.196829319000244\n",
      "epoch: 2500, loss: 1.9994860887527466\n",
      "epoch: 2600, loss: 1.8126541376113892\n",
      "epoch: 2700, loss: 1.6814367771148682\n",
      "epoch: 2800, loss: 1.4957882165908813\n",
      "epoch: 2900, loss: 1.3912500143051147\n",
      "epoch: 3000, loss: 1.2629814147949219\n",
      "epoch: 3100, loss: 1.211047887802124\n",
      "epoch: 3200, loss: 1.070693016052246\n",
      "epoch: 3300, loss: 1.0127036571502686\n",
      "epoch: 3400, loss: 0.9789557456970215\n",
      "epoch: 3500, loss: 0.8899975419044495\n",
      "epoch: 3600, loss: 0.8848996758460999\n",
      "epoch: 3700, loss: 0.8248185515403748\n",
      "epoch: 3800, loss: 0.7892289161682129\n",
      "epoch: 3900, loss: 0.7304442524909973\n",
      "epoch: 4000, loss: 0.6940990090370178\n",
      "epoch: 4100, loss: 0.6646054983139038\n",
      "epoch: 4200, loss: 0.6478905081748962\n",
      "epoch: 4300, loss: 0.6475481390953064\n",
      "epoch: 4400, loss: 0.6122651100158691\n",
      "epoch: 4500, loss: 0.6137274503707886\n",
      "epoch: 4600, loss: 0.5339426398277283\n",
      "epoch: 4700, loss: 0.5386178493499756\n",
      "epoch: 4800, loss: 0.5055069327354431\n",
      "epoch: 4900, loss: 0.5018178820610046\n",
      "epoch: 5000, loss: 0.491240918636322\n",
      "epoch: 5100, loss: 0.4782335162162781\n",
      "epoch: 5200, loss: 0.46895933151245117\n",
      "epoch: 5300, loss: 0.43598487973213196\n",
      "epoch: 5400, loss: 0.4281690716743469\n",
      "epoch: 5500, loss: 0.4227134585380554\n",
      "epoch: 5600, loss: 0.41864073276519775\n",
      "epoch: 5700, loss: 0.4176751375198364\n",
      "epoch: 5800, loss: 0.38820862770080566\n",
      "epoch: 5900, loss: 0.38223710656166077\n",
      "epoch: 6000, loss: 0.39257341623306274\n",
      "epoch: 6100, loss: 0.3706691563129425\n",
      "epoch: 6200, loss: 0.3616349995136261\n",
      "epoch: 6300, loss: 0.35348886251449585\n",
      "epoch: 6400, loss: 0.3598954677581787\n",
      "epoch: 6500, loss: 0.3519793450832367\n",
      "epoch: 6600, loss: 0.3407268226146698\n",
      "epoch: 6700, loss: 0.32982346415519714\n",
      "epoch: 6800, loss: 0.3352605700492859\n",
      "epoch: 6900, loss: 0.32229679822921753\n",
      "epoch: 7000, loss: 0.31611716747283936\n",
      "epoch: 7100, loss: 0.3171329200267792\n",
      "epoch: 7200, loss: 0.3052158057689667\n",
      "epoch: 7300, loss: 0.29823046922683716\n",
      "epoch: 7400, loss: 0.30613410472869873\n",
      "epoch: 7500, loss: 0.29468289017677307\n",
      "epoch: 7600, loss: 0.2975751757621765\n",
      "epoch: 7700, loss: 0.282882422208786\n",
      "epoch: 7800, loss: 0.2810501158237457\n",
      "epoch: 7900, loss: 0.2835482954978943\n",
      "epoch: 8000, loss: 0.27162882685661316\n",
      "epoch: 8100, loss: 0.2704927623271942\n",
      "epoch: 8200, loss: 0.262281596660614\n",
      "epoch: 8300, loss: 0.26489976048469543\n",
      "epoch: 8400, loss: 0.27093884348869324\n",
      "epoch: 8500, loss: 0.25997960567474365\n",
      "epoch: 8600, loss: 0.2533847987651825\n",
      "epoch: 8700, loss: 0.25818508863449097\n",
      "epoch: 8800, loss: 0.25181692838668823\n",
      "epoch: 8900, loss: 0.24732019007205963\n",
      "epoch: 9000, loss: 0.24601665139198303\n",
      "epoch: 9100, loss: 0.22992391884326935\n",
      "epoch: 9200, loss: 0.2395717054605484\n",
      "epoch: 9300, loss: 0.23265251517295837\n",
      "epoch: 9400, loss: 0.23297348618507385\n",
      "epoch: 9500, loss: 0.23014681041240692\n",
      "epoch: 9600, loss: 0.2319992184638977\n",
      "epoch: 9700, loss: 0.23103289306163788\n",
      "epoch: 9800, loss: 0.22200998663902283\n",
      "epoch: 9900, loss: 0.21861149370670319\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# train\n",
    "state_dim = data_s.shape[1]\n",
    "action_dim = data_a.shape[1]\n",
    "param_dim = data_param.shape[1]\n",
    "latent_dim = 2\n",
    "switch_dim = 5\n",
    "\n",
    "model_save_path = f'../data/weights/dynamics/inverteddoublependulum/'\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "updater = DynamicsParamsOptimizer(state_dim, action_dim, param_dim, latent_dim, switch_dim, model_save_path)\n",
    "updater.train(data_s, data_a, data_param, data_s_, epoch=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 11]) torch.Size([20, 11, 2]) torch.Size([20, 2])\n",
      "tensor([[ 2.2643, -1.8454],\n",
      "        [ 2.8811, -2.2793],\n",
      "        [ 3.5958, -2.9697],\n",
      "        [ 1.7742, -1.4963],\n",
      "        [ 2.4365, -2.0887],\n",
      "        [ 2.2297, -1.8889],\n",
      "        [ 1.8014, -1.4508],\n",
      "        [ 2.7828, -2.4257],\n",
      "        [ 3.4125, -2.8278],\n",
      "        [ 1.7107, -1.3469],\n",
      "        [ 2.0031, -1.6024],\n",
      "        [ 2.5392, -2.0994],\n",
      "        [ 3.7406, -3.1136],\n",
      "        [ 1.4040, -1.1620],\n",
      "        [ 2.3428, -1.9775],\n",
      "        [ 4.2024, -3.5203],\n",
      "        [ 1.4234, -1.1618],\n",
      "        [ 1.6157, -1.2830],\n",
      "        [ 1.8207, -1.4661],\n",
      "        [ 2.3034, -1.9059]], grad_fn=<ViewBackward>) tensor([[ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811],\n",
      "        [ 2.3339, -1.8811]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_size = 20\n",
    "test_s = data_s[:test_size]\n",
    "test_a = data_a[:test_size]\n",
    "test_param = data_param[:test_size]\n",
    "test_s_ = data_s_[:test_size]\n",
    "\n",
    "alpha = updater.model.get_latent_code(test_s, test_a, test_s_)\n",
    "\n",
    "# compare with encoded value\n",
    "alpha_ = torch.Tensor(test_param)@updater.model.E\n",
    "print(alpha, alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cace607863cb2b54a92bea130d421e24d3df8ef9f71a9225e8b737b7ba466912"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('rlgpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
