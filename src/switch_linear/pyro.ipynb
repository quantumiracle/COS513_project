{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from torch.distributions import constraints\n",
    "from pyro.nn import PyroModule, PyroParam, PyroSample\n",
    "from pyro.nn.module import to_pyro_module_\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_size, out_size))\n",
    "        self.bias = nn.Parameter(torch.randn(out_size))\n",
    "\n",
    "    def forward(self, input_):\n",
    "        return self.bias + input_ @ self.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.8123, -1.1436], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(2,2)\n",
    "to_pyro_module_(linear)  # to pyro module: this operates in-place\n",
    "\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6131, -2.0254],\n",
      "        [ 2.5195, -2.0059],\n",
      "        [ 1.0724, -0.9205]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "example_input = torch.randn(3, 2)\n",
    "example_output = linear(example_input)\n",
    "print(example_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinear(PyroModule):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        # self.weight = PyroSample(dist.Normal(0, 1).expand([in_size, out_size]).to_event(2))\n",
    "        self.weight = torch.randn(in_size, out_size)  # replace with some fixed constant will just not update these values\n",
    "        # self.weight = PyroModule[nn.Linear](in_size, out_size) # this will not work\n",
    "        self.bias = PyroSample(dist.Normal(0, 1).expand([out_size]).to_event(1))\n",
    "\n",
    "    def forward(self, input_):\n",
    "        return self.bias + input_ @ self.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0048, -1.9638],\n",
      "        [-0.1089, -0.6803],\n",
      "        [-2.0456, -0.1828]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "blinear = Linear(2,2)\n",
    "x = torch.randn(3, 2)\n",
    "y = linear(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(PyroModule):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.linear = BayesianLinear(in_size, out_size)  # this is a PyroModule\n",
    "        self.obs_scale = PyroSample(dist.LogNormal(0, 1))\n",
    "        # self.obs_scale = pyro.sample(\"sigma\", dist.Uniform(0., 1.).expand([1]).to_event(1))\n",
    "\n",
    "    def forward(self, input, output=None):\n",
    "        obs_loc = self.linear(input)  # this samples linear.bias and linear.weight\n",
    "        obs_scale = self.obs_scale    # this samples self.obs_scale\n",
    "        with pyro.plate(\"instances\", len(input)):\n",
    "            return pyro.sample(\"obs\", dist.Normal(obs_loc, obs_scale).to_event(1),\n",
    "                               obs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5912,  0.2738],\n",
      "        [-0.9649, -0.2358],\n",
      "        [ 1.8793, -0.0721],\n",
      "        [ 0.1578, -0.7735],\n",
      "        [ 0.1991,  0.0457],\n",
      "        [ 0.1530, -0.4757],\n",
      "        [-0.1110,  0.2927],\n",
      "        [-0.1578, -0.0288],\n",
      "        [ 2.3571, -1.0373],\n",
      "        [ 1.5748, -0.6298]]) tensor([[-0.9274,  0.5451,  0.0663],\n",
      "        [-0.4370,  0.7626,  0.4415],\n",
      "        [ 1.1651,  2.0154,  0.1374],\n",
      "        [ 0.9386, -0.1860, -0.6446],\n",
      "        [ 1.5392, -0.8696,  0.2579],\n",
      "        [ 1.0950, -0.5065,  0.0998],\n",
      "        [-0.6540,  0.7317, -1.4344],\n",
      "        [-0.5008,  0.0938, -1.2597],\n",
      "        [ 0.2546, -0.5020, -1.0412],\n",
      "        [ 0.7323, -1.0483, -0.4709]]) 30\n",
      "step 0 loss = 1.617\n",
      "step 100 loss = 1.522\n",
      "step 200 loss = 1.489\n",
      "step 300 loss = 1.5\n",
      "step 400 loss = 1.468\n",
      "step 500 loss = 1.493\n",
      "step 600 loss = 1.491\n",
      "step 700 loss = 1.501\n",
      "step 800 loss = 1.474\n",
      "step 900 loss = 1.488\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(1)\n",
    "\n",
    "in_size, out_size = 2,3\n",
    "model = Model(in_size,out_size)\n",
    "x = torch.randn(10, in_size)\n",
    "y = torch.randn(10, out_size)\n",
    "\n",
    "guide = AutoNormal(model)  # unlearned posterior dist. AutoDiagonalNormal\n",
    "svi = SVI(model, guide, Adam({\"lr\": 0.01}), Trace_ELBO())  # parameters to optimize are determined by guide()\n",
    "print(x,y,y.numel())\n",
    "for step in range(1000):\n",
    "    loss = svi.step(x, y) / y.numel()  # data in step() are passed to both model() and guide()\n",
    "    if step % 100 == 0:\n",
    "        # print(model.linear.weight)\n",
    "        print(\"step {} loss = {:0.4g}\".format(step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoNormal.locs.linear.bias Parameter containing:\n",
      "tensor([-1.2041])\n",
      "AutoNormal.scales.linear.bias tensor([0.2056])\n",
      "AutoNormal.locs.obs_scale Parameter containing:\n",
      "tensor(-0.8343)\n",
      "AutoNormal.scales.obs_scale tensor(0.4281)\n"
     ]
    }
   ],
   "source": [
    "# examine the optimized parameter values\n",
    "guide.requires_grad_(False)\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obs': {'mean': tensor([[-0.2483],\n",
      "        [-1.2851]]), 'std': tensor([[0.6681],\n",
      "        [0.5240]]), '5%': tensor([[-1.3640],\n",
      "        [-2.5116]]), '95%': tensor([[ 0.8016],\n",
      "        [-0.6935]])}, '_RETURN': {'mean': tensor([[-0.2483],\n",
      "        [-1.2851]]), 'std': tensor([[0.6681],\n",
      "        [0.5240]]), '5%': tensor([[-1.3640],\n",
      "        [-2.5116]]), '95%': tensor([[ 0.8016],\n",
      "        [-0.6935]])}}\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trained model: posterior dist.\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "\n",
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for k, v in samples.items():\n",
    "        site_stats[k] = {\n",
    "            \"mean\": torch.mean(v, 0),\n",
    "            \"std\": torch.std(v, 0),\n",
    "            \"5%\": v.kthvalue(int(len(v) * 0.05), dim=0)[0],\n",
    "            \"95%\": v.kthvalue(int(len(v) * 0.95), dim=0)[0],\n",
    "        }\n",
    "    return site_stats\n",
    "\n",
    "x_data = torch.randn(2, in_size)\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=20,\n",
    "                        return_sites=(\"linear.weight\", \"obs\", \"_RETURN\"))\n",
    "samples = predictive(x_data)\n",
    "pred_summary = summary(samples)\n",
    "# print(samples)\n",
    "print(pred_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear.weight': {'mean': tensor([[[-0.1136],\n",
      "         [-0.4577]]]), 'std': tensor([[[0.6488],\n",
      "         [0.6626]]]), '5%': tensor([[[-1.1910],\n",
      "         [-1.5464]]]), '95%': tensor([[[0.9701],\n",
      "         [0.6256]]])}, 'obs': {'mean': tensor([[-1.2794]]), 'std': tensor([[1.0144]]), '5%': tensor([[-2.9126]]), '95%': tensor([[0.3741]])}, '_RETURN': {'mean': tensor([[-1.2794]]), 'std': tensor([[1.0144]]), '5%': tensor([[-2.9126]]), '95%': tensor([[0.3741]])}}\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.randn(1, in_size)\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=8000,\n",
    "                        return_sites=(\"linear.weight\", \"obs\", \"_RETURN\"))\n",
    "samples = predictive(x_data)\n",
    "pred_summary = summary(samples)\n",
    "# print(samples)\n",
    "print(pred_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7244, -0.7022],\n",
      "        [ 1.1661,  0.2605]]) tensor([0.3460, 0.2065], grad_fn=<SqueezeBackward1>) 2\n",
      "Parameter containing:\n",
      "tensor([[ 0.3643, -0.3121]], requires_grad=True)\n",
      "step 0 loss = 3.812\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3621/2050572269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# data in step() are passed to both model() and guide()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/x/lib/python3.7/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         params = set(\n",
      "\u001b[0;32m~/anaconda3/envs/x/lib/python3.7/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             ):\n\u001b[1;32m    156\u001b[0m                 \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0msurrogate_loss_particle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mwarn_if_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/x/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/x/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "class BayesianRegression(PyroModule):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = PyroModule[nn.Linear](in_features, out_features)\n",
    "        # self.linear.weight = PyroSample(dist.Normal(0., 1.).expand([out_features, in_features]).to_event(2))\n",
    "        # self.linear.bias = PyroSample(dist.Normal(0., 10.).expand([out_features]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "        mean = self.linear(x).squeeze(-1)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs1\", dist.Normal(mean, sigma), obs=y)\n",
    "        return mean\n",
    "\n",
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(1)\n",
    "\n",
    "in_size, out_size = 2,1\n",
    "model = BayesianRegression(in_size,out_size)\n",
    "x = torch.randn(2, in_size)\n",
    "y = model(x)\n",
    "\n",
    "guide = AutoNormal(model)  # unlearned posterior dist. AutoDiagonalNormal\n",
    "svi = SVI(model, guide, Adam({\"lr\": 0.01}), Trace_ELBO())  # parameters to optimize are determined by guide()\n",
    "print(x,y,y.numel())\n",
    "for step in range(1000):\n",
    "    loss = svi.step(x, y) / y.numel()  # data in step() are passed to both model() and guide()\n",
    "    if step % 100 == 0:\n",
    "        print(model.linear.weight)\n",
    "        print(\"step {} loss = {:0.4g}\".format(step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6861e59928cb790236f7047915368f37afc12f670e78fd0101a6f825a02b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('x': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
